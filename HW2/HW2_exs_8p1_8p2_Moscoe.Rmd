---
title: "KJ exs 8.1, 8.2"
author: "Daniel Moscoe"
date: "6/30/2021"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE
  )
```

## Exercise 8.1

#### Question

"Recreate the simulated data from Exercise 7.2:

```{r}
library(mlbench)
library(tidyverse)
set.seed(200)
simulated <- mlbench.friedman1(200, sd = 1)
simulated <- cbind(simulated$x, simulated$y)
simulated <- as.data.frame(simulated)
colnames(simulated)[ncol(simulated)] <- "y"
```

"a. Fit a random forest model to all of the predictors, then estimate the variable importance scores:

```{r}
library(randomForest)
library(caret)
model1 <- randomForest(y ~ ., data = simulated,
                       importance = TRUE,
                       ntree = 1000)
rfImp1 <- varImp(model1, scale = FALSE)
```

"Did the random forest model significantly use the uninformative predictors (V6-V10)?"

#### Code

```{r}
rfImp1
```

#### Response

The code above shows the overall variable importance scores for the model. Variables V6-V10 have very low scores relative to the informative variables, V1-V5. The random forest model did not significantly use the uninformative predictors.

#### Question

"b. Now add an additional predictor that is highly correlated with one of the informative predictors.... Fit another random forest model to these data. Did the importance score for V1 change?"

#### Code

```{r}
simulated2 <- mutate(simulated, "duplicate1" = simulated$V1 + rnorm(200) * 0.1)
model2 <- randomForest(y ~ ., data = simulated2,
                       importance = TRUE,
                       ntree = 1000)
rfImp2 <- varImp(model2, scale = FALSE)
rfImp2
```

#### Response

The importance score for V1 is reduced when another highly correlated predictor variable is added to the model. This example serves as a warning about interpreting results of random forests that contain correlated variables. When several variables are highly correlated, it's sometimes more informative to measure their importance together as a group. Or, a measure of variable importance such as Strobl et al.'s could be used instead (see p. 202).

#### Question

"What happens when you add another predictor that is also highly correlated with V1?"

#### Code

```{r}
simulated3 <- mutate(simulated2, "duplicate2" = simulated$V1 + rnorm(200) * 0.1)
model3 <- randomForest(y ~ ., data = simulated3,
                       importance = TRUE,
                       ntree = 1000)
rfImp3 <- varImp(model3, scale = FALSE)
rfImp3
```

#### Response

Because duplicate2 is highly correlated with both V1 and duplicate1, adding the second duplicate predictor reduces the variable importance scores of both V1 and the first duplicate, duplicate1. 

#### Question

"c. Use the `cforest` function in the `party` package to fit a random forest model using conditional inference trees. The `party` package function `varimp` can calculate predictor importance. The `conditional` argument of that function toggles between the traditional importance measure and the modified version described in Strobl et al. (2007). Do these importances show the same pattern as the traditional random forest model?"

#### Code

```{r}
library(partykit)

#CI tree model for original data
model4 <- cforest(y ~ ., data = simulated, ntree = 1000)

#CI tree model with 1 additional uninformative variable
model5 <- cforest(y ~ ., data = simulated2, ntree = 1000)

#CI tree model with 2 additional uninformative variables
model6 <- cforest(y ~ ., data = simulated3, ntree = 1000)

#Variable importance scores with conditional = TRUE
cfImp4.con <- varimp(model4, conditional = TRUE)
cfImp5.con <- varimp(model5, conditional = TRUE)
cfImp6.con <- varimp(model6, conditional = TRUE)

#Variable importance scores with conditional = FALSE
cfImp4.unc <- varimp(model4, conditional = FALSE)
cfImp5.unc <- varimp(model5, conditional = FALSE)
cfImp6.unc <- varimp(model6, conditional = FALSE)
```

```{r, echo = FALSE}
print("No add'l var's, conditional = TRUE")
print(cfImp4.con)
```

```{r, echo = FALSE}
print("1 add'l var's, conditional = TRUE")
print(cfImp5.con)
```

```{r, echo = FALSE}
print("2 add'l var's, conditional = TRUE")
print(cfImp6.con)
```

```{r, echo = FALSE}
print("No add'l var's, conditional = FALSE")
print(cfImp4.unc)
```

```{r, echo = FALSE}
print("1 add'l var's, conditional = FALSE")
print(cfImp5.unc)
```

```{r, echo = FALSE}
print("2 add'l var's, conditional = FALSE")
print(cfImp6.unc)
```

#### Response

As additional variables highly correlated to V1 were added to the model, the importance of V1 declined for both traditional and modified importance measures. For the traditional importance measure, the importance of V1 declined 51% and then 26% with one and two additional predictors, respectively. For the modified importance measure, the importance of V1 declined less: 27% with one additional variable, and an additional 8% when a second highly correlated variable was added to the model. Total decline in the importance of V1 under the traditional importance measure is 64%, and total decline in the importance of V1 under the modified importance measure is 33%. The modified importance measure mitigated the decline in variable importance associated with the addition of highly correlated predictors.

#### Question

"d. Repeat this process with different tree models, such as boosted trees and Cubist. Does the same pattern occur?"

#### Code

```{r, fig.show = 'hide'}
#Boosted tree model for original data
library(gbm)
model7 <- gbm(y ~ ., data = simulated, distribution = "gaussian")

#Boosted tree model with 1 additional uninformative variable
model8 <- gbm(y ~ ., data = simulated2, distribution = "gaussian")

#Boosted tree model with 2 additional uninformative variables
model9 <- gbm(y ~ ., data = simulated3, distribution = "gaussian")

#Variable importance scores with method = relative.influence
btImp7.ri <- summary.gbm(object = model7, method = relative.influence)
btImp8.ri <- summary.gbm(object = model8, method = relative.influence)
btImp9.ri <- summary.gbm(object = model9, method = relative.influence)

#Variable importance scores with conditional = FALSE
btImp7.pt <- summary.gbm(object = model7, method = permutation.test.gbm)
btImp8.pt <- summary.gbm(object = model8, method = permutation.test.gbm)
btImp9.pt <- summary.gbm(object = model9, method = permutation.test.gbm)
```

```{r, echo = FALSE}
print("No add'l var's, method = relative.importance")
print(btImp7.ri)
```


```{r, echo = FALSE}
print("1 add'l var's, method = relative.importance")
print(btImp8.ri)
```

```{r, echo = FALSE}
print("2 add'l var's, method = relative.importance")
print(btImp9.ri)
```

```{r, echo = FALSE}
print("No add'l var's, method = relative.permutation.test.gbm")
print(btImp7.pt)
```

```{r, echo = FALSE}
print("1 add'l var's, method = relative.permutation.test.gbm")
print(btImp8.pt)
```

```{r, echo = FALSE}
print("2 add'l var's, method = relative.permutation.test.gbm")
print(btImp9.pt)
```

#### Response

For boosted trees, there are two methods for assessing variable importance: `relative.influence`, which is a traditional procedure, and an experimental procedure called `permutation.test.gbm`. As additional variables highly correlated with V1 are added to the model, the relative importance of V1 declines more steeply under the `permutation.test.gbm` method than under the `relative.influence` method. The importance of V1 declines a total of 51% under the relative importance measure when two additional variables are added. Under the permutation test, the total decline in importance for V1 is 60%.

#### Code

```{r}
#Cubist Model
library(Cubist)

model10 <- cubist(x = simulated[-11], y = simulated$y, committees = 100)
model11 <- cubist(x = simulated2[-11], y = simulated2$y, committees = 100)
model12 <- cubist(x = simulated3[-11], y = simulated3$y, committees = 100)
```

```{r}
#Note: output is truncated to emphasize measures of variable importance.
summary(model10)
summary(model11)
summary(model12)
```

#### Response

While "There is no established technique for measuring predictor importance for Cubist models" (KJ 212), we can observe the frequency with which a variable appeared in a rule criterion ("conds"). This gives some indication of the importance of a variable during the Cubist modeling process.  

In the first model, which does not include additional variables correlated with `V1`, `V1` appeared in rule criteria 47% of the time. As `duplicate1` and then `duplicate2` were added to the model, `V1` appeared in rule criteria 38% and 43% of the time, respectively. This indicates some decline in variable importance for `V1` as highly correlated variables are added to the model. In terms of its appearance in final models, `V1` underwent a more significant decline: from 96% with no correlated duplicate variables, to 67%, and then to 58%.

## Exercise 8.2

#### Question

"Use a simulation to show tree bias with different granularities."

#### Code

```{r}
#Create dataframe with some informative (x11, x12, x13) and some uninformative (x14, x15, x16) predictors.

y <- rnorm(1000, 0, 1)
x11 <- y + rnorm(1000, 1, 0.5)
x12 <- y / 3 * rnorm(1000, 0, 0.5)
x13 <- (y + rnorm(1000, 0, 0.5)) ^ (1/3) 
x14 <- rnorm(1000, 0, 1)
x15 <- runif(1000, 4, 6)
x16 <- rexp(1000, 0.5)
df_1 <- data.frame(x11 = x11, x12 = x12, x13 = x13, x14 = x14, x15 = x15, x16 = x16, y = y)

#Generate cutpoints for decreasing granularity of uninformative predictors (50, 20, and 10 distinct values).

x14_50 <- seq(from = min(df_1$x14), to = max(df_1$x14), length.out = 50)
x14_20 <- seq(from = min(df_1$x14), to = max(df_1$x14), length.out = 20)
x14_10 <- seq(from = min(df_1$x14), to = max(df_1$x14), length.out = 10)

x15_50 <- seq(from = min(df_1$x15), to = max(df_1$x15), length.out = 50)
x15_20 <- seq(from = min(df_1$x15), to = max(df_1$x15), length.out = 20)
x15_10 <- seq(from = min(df_1$x15), to = max(df_1$x15), length.out = 10)

x16_50 <- seq(from = min(df_1$x16), to = max(df_1$x16), length.out = 50)
x16_20 <- seq(from = min(df_1$x16), to = max(df_1$x16), length.out = 20)
x16_10 <- seq(from = min(df_1$x16), to = max(df_1$x16), length.out = 10)

#Create low-granularity variables

x24 <- as.numeric(as.vector(cut(df_1$x14, breaks = 50, labels = x14_50)))
x34 <- as.numeric(as.vector(cut(df_1$x14, breaks = 20, labels = x14_20)))
x44 <- as.numeric(as.vector(cut(df_1$x14, breaks = 10, labels = x14_10)))
x25 <- as.numeric(as.vector(cut(df_1$x15, breaks = 50, labels = x15_50)))
x35 <- as.numeric(as.vector(cut(df_1$x15, breaks = 20, labels = x15_20)))
x45 <- as.numeric(as.vector(cut(df_1$x15, breaks = 10, labels = x15_10)))
x26 <- as.numeric(as.vector(cut(df_1$x16, breaks = 50, labels = x16_50)))
x36 <- as.numeric(as.vector(cut(df_1$x16, breaks = 20, labels = x16_20)))
x46 <- as.numeric(as.vector(cut(df_1$x16, breaks = 10, labels = x16_10)))

#Construct reduced granularity dataframes

df_2 <- data.frame(x11 = x11, x12 = x12, x13 = x13, x24 = x24, x25 = x25, x26 = x26, y = y)
df_3 <- data.frame(x11 = x11, x12 = x12, x13 = x13, x34 = x34, x35 = x35, x36 = x36, y = y)
df_4 <- data.frame(x11 = x11, x12 = x12, x13 = x13, x44 = x44, x45 = x45, x46 = x46, y = y)

#Construct models

library(rpart)
df_1.mod <- rpart(y ~ ., data = df_1)
df_2.mod <- rpart(y ~ ., data = df_2)
df_3.mod <- rpart(y ~ ., data = df_3)
df_4.mod <- rpart(y ~ ., data = df_4)
```

```{r, echo = FALSE}
print("Variable importance with high granularity uninformative vars")
print(varImp(df_1.mod))
```

```{r, echo = FALSE}
print("Variable importance with uninformative vars (50 distinct vals)")
varImp(df_2.mod)
```

```{r, echo = FALSE}
print("Variable importance with uninformative vars (20 distinct vals)")
varImp(df_3.mod)
```

```{r, echo = FALSE}
print("Variable importance with uninformative vars (10 distinct vals")
varImp(df_4.mod)
```

#### Response

Tree bias due to granularity occurs when a variable's importance in a tree model is inflated not due to its strong relationship to the response, but merely because of the large number of distinct values that variable takes on. Variables that take on large numbers of distinct values are said to be granular. To demonstrate tree bias due to granularity, we examined a single dataset comprised of some informative and some uninformative variables. By binning the uninformative variables (first to 50 distinct values, then 20, then 10) we were able to reduce their granularity. If bias due to granularity exists in the original high-granularity data, then the variable importance of the uninformative variables should decrease as they are binned to fewer and fewer distinct values.

The plots below show the relationship between X14, an uninformative variable, and the response variable, as the granularity of x14 is decreased.

```{r}
plot(df_1$x14, df_1$y, xlab = "x14, high granularity", ylab = "y", main = "x14 vs. response (high gran)")
plot(df_2$x24, df_2$y, xlab = "x14, 50 distinct vals", ylab = "y", main = "x14 vs. response (50 distinct vals)")
plot(df_3$x34, df_3$y, xlab = "x14, 20 distinct vals", ylab = "y", main = "x14 vs. response (20 distinct vals)")
plot(df_4$x44, df_4$y, xlab = "x14, 10 distinct vals", ylab = "y", main = "x14 vs. response (10 distinct vals)")
```

The code in the **Code** section above shows the construction of the original dataset, along with the process for reducing the granularity of each uninformative variable through binning. This code is followed by variable importance tables for each model as granularity is varied.

As the granularity of the non-explanatory variables decreased, the overall importance of those variables decreased as well. This confirms that increased granularity can bias a tree model toward predictors with greater granularity, even when those predictors contain no information about the response variable.