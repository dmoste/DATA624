---
title: "draft_proj1_210622"
author: "Daniel Moscoe"
date: "6/22/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(fpp2)
library(readxl)
```

```{r}
xlsx_path <- "raw_data.xlsx"
raw <- readxl::read_xlsx(xlsx_path)
summary(raw)
```

```{r}

raw %>%
#  filter(S06Var07 < 50) %>%
  ggplot(aes(x = SeriesInd, y = S06Var05)) +
    geom_line()
```

```{r}
ggplot(data = raw, aes(x = SeriesInd, y = S06Var07)) +
  geom_line()
```

```{r}
ggplot(data = as.data.frame(tmpdiffs), aes(x = seq(1:length(tmpdiffs)), y = tmpdiffs)) +
  geom_point()
```

S06Var05 and S06Var07 are very closely related. It looks like their non-lagged differences are mean 0, normally distributed, constant variance. Whatever model works for S06Var05 should also be applied to S06Var07.

Initial thoughts:
- Ignore the extreme outliers.
- Looks like a random walk with drift.
- Is there seasonality?

```{r}
S06Var05.ts <- ts(raw$S06Var05)
autoplot(S06Var05.ts)
```

Trend
Cyclicity
Seasonality
Residual

Does converting this to a ts object ignore missing values? Are the missing values meaningful? If the variability of the values on either side of the missing regions is equal to the average variability, then the time series is stopped for those times, and you can drop the missing values. If the variability is greater, then the data is actually missing.

```{r}
autoplot(diff(S06Var05.ts))
```

Except for extreme outliers, the differences are mean 0 and it looks like their variability is related to the level of the ts.

So here's my plan right now:

*Check whether missingness is meaningful by comparing variability around missing periods with average variability.

*Check whether this fits a decomposition model and run it. What kind of decomposition will you choose? Produce forecasts.

*Then do an ARIMA model with first differences. Produce forecasts.

*Decide on one model.

### Check whether missingness is meaningful

```{r}
SeriesInd.ts <- ts(raw$SeriesInd)
gaps <- diff(SeriesInd.ts) > 1
gaps <- c(FALSE, gaps)
tmp <- as.vector(diff(SeriesInd.ts))
filter(tmp > 1) %>%
  
```

I want to get a list of data that occurs only directly before or after a gap. So first I need to identify where the gaps are. This is `gaps`. However, gaps is not matched with SeriesInd. So I need to get them together in the same data frame.

```{r}
gaps.df <- data.frame("SeriesInd" = raw$SeriesInd, "AfterGap" = gaps)
head(gaps.df)
```

Great. Now I have SeriesInd labeled with whether a row follows a gap. So I need to take all the values matching AfterGap, along with their lags, and compare the variance.

Just take the differences at AfterGap == True.

```{r}
gaps.df <- gaps.df %>%
  mutate("S06Var05" = raw$S06Var05, "S06Var07" = raw$S06Var07, "S06Var05.diff" = raw$S06Var05 - lag(raw$S06Var05), "S06Var07.diff" = raw$S06Var07 - lag(raw$S06Var07))
```

Variance across gaps:
```{r}
variance_across_gaps_S06Var05 <- gaps.df %>%
  filter(AfterGap) %>%
  filter(S06Var05.diff > -50) %>%
  select(S06Var05.diff) %>%
  var(na.rm = TRUE)

variance_across_gaps_S06Var07 <- gaps.df %>%
  filter(AfterGap) %>%
  filter(S06Var07.diff > -50) %>%
  select(S06Var07.diff) %>%
  var(na.rm = TRUE)
```

Compare to variance of all diffs:
```{r}
variance_S06Var05 <- gaps.df %>%
#  filter(AfterGap) %>%
  filter(abs(S06Var05.diff) < 50) %>%
  select(S06Var05.diff) %>%
  var(na.rm = TRUE)

variance_S06Var07 <- gaps.df %>%
#  filter(AfterGap) %>%
  filter(abs(S06Var07.diff) < 50) %>%
  select(S06Var07.diff) %>%
  var(na.rm = TRUE)
```

Barring outliers, the variances of diffs across gaps are almost equal to the variances of all diffs. So I would say that the gaps don't matter, and we can treat each variable like a time series with no missingness except for cells that are marked NA.

So now I'm going to examine S06Var05 and S06Var07 as time series objects.

```{r}
S06Var05.ts <- ts(raw$S06Var05)
S06Var07.ts <- ts(raw$S06Var07)
sum(is.na(S06Var05.ts))
sum(is.na(S06Var07.ts))
```
They both still have 145 missing values. Are these because of the gaps in SeriesInd?

```{r}
sum(gaps.df$AfterGap)
```

No, there were 379 gaps.

I'm going to try just removing the NAs.

Drop all NAs:
```{r}
S06Var05.ts <- raw %>%
  select(S06Var05) %>%
  filter(S06Var05 < 100) %>%
  drop_na() %>%
  ts()

S06Var07.ts <- raw %>%
  select(S06Var07) %>%
  filter(S06Var07 < 100) %>%
  drop_na() %>%
  ts()
```

I don't know if I treated the NAs exactly the right way... maybe better would have been to smoothly impute over them. But for now I'm just going to move on with my model. If I want to rerun the model later with the NAs imputed, that should be doable.

*Check whether this fits a decomposition model and run it. What kind of decomposition will you choose? Produce forecasts.

```{r}
autoplot(S06Var05.ts)
```


As it stands, S06Var05.ts has a frequency of 1. I have a hunch this data is prices for 
stocks not traded on weekends/holidays. So I will set the frequency to 52 to enable a seasonal 
decomposition, just to see what happens.
```{r}
ts(S06Var05.ts, frequency = 30) %>% decompose(type = 'multiplicative') %>%
  autoplot() + xlab('Time') +
  ggtitle('Classical multiplicative decomp S06Var05')
```

A classical decomposition doesn't seem like a good model. The seasonal component is very small relative to the trend component and the level of the data. Varying the frequency of the time series didn't change this. Let's turn to exponential smoothing / methods based on exponential smoothing. The tiny seasonal component isn't going to change with some other decomposition method, even though other methods have other advantages.

Exponential smoothing: Holt:

```{r}
holt_undamped <- holt(S06Var05.ts, h = 500)
holt_damped <- holt(S06Var05.ts, damped = TRUE, phi = 0.9, h = 500)
autoplot(S06Var05.ts) +
  autolayer(holt_undamped, PI = FALSE) +
  autolayer(holt_damped, series = "damped", PI = FALSE)
```

This might be promising. The undamped forecast line seems plausible. Actually, both seem plausible. This is a model to come back to and refine after you have a few other candidate models.

There are other exponential smoothing models that you might want to try.

*Then do an ARIMA model with first differences. Produce forecasts.

```{r}
autoplot(Acf(diff(S06Var05.ts)))
```

There are many lags with autocorrelation here. The differenced data is not stationary, as evidenced by this ACF plot and a Box-Ljung test below.

```{r}
autoplot(diff(S06Var05.ts))
```

Odd, because the graph definitely looks like stationary data.

Second-order differences:

```{r}
autoplot(Acf(diff(S06Var05.ts, differences = 2)))
```

```{r}
Box.test(diff(S06Var05.ts, differences = 2), type = 'Ljung-Box')
```
Still definitely not stationary. But why does this matter?
"We normally restrict autoregressive models to stationary data" (8.3).
"The same stationarity and invertibility conditions that are used for autoregressive and moving average models also apply to an ARIMA model."
```{r}
autoplot(Pacf(S06Var05.ts))
```

The PACF plot also shows several lags with significant autocorrelation. What does that mean? It seems like autocorrelation should make the model easier to fit, since we can get more information from previous values. But somehow I feel like we're supposed to see stationary data. Also, this raises again the question of what is stationary data. 

```{r}
Box.test(diff(S06Var05.ts), lag = 1, type = 'Ljung-Box')
```

The p value is very small, which means that the differenced data is not stationary. Why does this matter?

```{r}
auto_arima_fit <- auto.arima(S06Var05.ts)
summary(auto_arima_fit)
```

```{r}
checkresiduals(auto_arima_fit)
```

Thinking back to assessing the missing data: you don't care whether the variability of the differences across gaps is greater... you care if the average difference across gaps is greater. You need to look at levels, not variances.

These residuals seem okay... you have those few nasty outliers.

How do you know if this model is good or not?

```{r}
tmp_arima <- Arima(S06Var05.ts, order = c(2,1,2), include.drift = TRUE)
autoplot(forecast(tmp_arima, h = 200))
```

Okay. It's time to check conditions on whether an ARIMA model is appropriate, and then fit one.

I don't know that an ARIMA model is appropriate, because the data is not stationary, and the differenced data of order 1 and order 2 are not stationary. ARIMA models assume stationarity. So I'm going to go back to Exponential smoothing.

- Simple exponential smoothing is for data without trend. But this data clearly has trend.

```{r}

wdw <- window(S06Var05.ts, start = 1000)

exp_smth <- ses(wdw, h = 20)
autoplot(wdw) +
  autolayer(fitted(exp_smth), series = "Fitted")
```

That is very lame.

Go back to damped Holt's again: (section 7.2)

```{r}
#Starting with an automatically determined model:
holt_damped <- holt(S06Var05.ts, damped = TRUE, phi = 0.8)
autoplot(wdw) +
  autolayer(holt_damped, series = "damped", PI = TRUE)
```

Why is the trend completely lost here? What is phi?

```{r}
summary(holt_damped)
```

phi = 0.9733, so yes, very close to 1 when determined automatically. But even when I set it to 0.8, there's no noticeable trend. "We have set the damping parameter to a relatively low number (phi = 0.90)... usually we would estimate phi along with the other parameters" (7.2). 

"Use time-series cross-validation to compare the one-step forecast accuracy of the three methods" (7.2).

```{r}
e1 <- tsCV(S06Var05.ts, ses, h = 1)
e2 <- tsCV(S06Var05.ts, holt, h = 1)
e3 <- tsCV(S06Var05.ts, holt, damped = TRUE, h = 1)

#Compare MSE:
mean(e1^2, na.rm=TRUE)

mean(e2^2, na.rm=TRUE)

mean(e3^2, na.rm=TRUE)

# Compare MAE:
mean(abs(e1), na.rm=TRUE)

mean(abs(e2), na.rm=TRUE)

mean(abs(e3), na.rm=TRUE)
```

Straight exponential smoothing has the lowest MAE and MSE.

Yes, all those NAs are at the end. I thought I dropped those rows, but now I remember I kept them in because the SeriesInd values for those rows are not strictly sequential.