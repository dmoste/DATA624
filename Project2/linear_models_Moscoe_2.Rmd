---
title: 'DATA 624 Proj 2: Linear Models'
author: "Daniel Moscoe"
date: "7/8/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(corrplot)
library(pls)
set.seed(0707)
```

## Import

```{r}
initial_import.df <- read.csv("https://raw.githubusercontent.com/dmoste/DATA624/main/Project2/StudentDataTOMODEL.csv")
to_predict.df <- read.csv("https://raw.githubusercontent.com/dmoste/DATA624/main/Project2/StudentEvaluationTOPREDICT.csv")
```

## Tidy

```{r}
#Drop missing PH rows
initial_import.df <- initial_import.df %>%
  filter(!is.na(PH))

#Separate predictors, response
preds.df <- initial_import.df[,-26]
resp.df <- initial_import.df[,26]
```

## Transform

```{r}
#Impute missing values with medians
brand_code <- preds.df[,1]
preds.df <- lapply(preds.df[,2:ncol(preds.df)], function(x) replace(x, is.na(x), median(x, na.rm = TRUE)))
preds.df <- as.data.frame(preds.df)
preds.df$`Brand.Code` <- brand_code

#Impute missing Brand Code as "C"
brand.code_na <- preds.df$Brand.Code == ""
preds.df[brand.code_na,32] <- "C"

#Drop low-variance variable
preds.df <- preds.df[,-12]

#Center / scale / Box-Cox
trans <- preProcess(preds.df, method = c("center", "scale", "BoxCox"))
preds.df <- predict(trans, preds.df)

#Split into train/test
training_rows <- sample(nrow(preds.df), nrow(preds.df) * 0.80, replace = FALSE)
train_preds.df <- preds.df[training_rows,]
train_resp.df <- resp.df[training_rows]
test_preds.df <- preds.df[-training_rows,]
test_resp.df <- resp.df[-training_rows]
```

## Model: Ordinary Least Squares

Since this section will be about linear models, let's search for highly correlated variables. While removing these variables doesn't guarantee an absence of multicollinearity, it is a useful first step, and can reduce the total number of variables in the model.

```{r}
corr_matrix <- cbind(train_preds.df, train_resp.df) %>%
  keep(is.numeric) %>%
  cor(method = "pearson")
corrplot::corrplot(corr_matrix, method = "circle", is.corr = TRUE)
```

Pairs of variables that are highly correlated: `Alch Rel` `Density`, `Balling`, `Carb Rel`, `Balling Lvl`;
`Air Pressurer` `Carb Rel`, `Balling Lvl`;
`Bowl Setpoint`, `Filler Level`;
`Filler Speed`, `MFR`;
`Hyd Pressure2`, `Hyd Pressure3`;
`Carb Temp`, `Carb Pressure`;

Keep `Balling` and drop `Balling Lvl` and Density;
Keep `Alch Rel` and drop `Carb Rel`;
Keep `Bowl Setpoint` and drop `Filler Level`;
Keep `MFR` and drop `Filler Speed`;
Keep `Hyd Pressure2` and drop `Hyd Pressure3`;
Keep `Carb Pressure` and drop `Carb Temp`.

```{r}
train_preds2.df <- train_preds.df %>%
  select(-`Balling.Lvl`,
         -Density,
         -`Carb.Rel`,
         -`Filler.Level`,
         -`Filler.Speed`,
         -`Hyd.Pressure3`,
         -`Carb.Temp`)

corr_matrix <- cbind(train_preds2.df, train_resp.df) %>%
  keep(is.numeric) %>%
  cor(method = "pearson")

corrplot::corrplot(corr_matrix, method = "circle", is.corr = TRUE)

```

There are still some large correlations remaining, for example, between Balling and `Alch Rel`. But because I know Balling is a measure of sugar content, and I expect that sugar content is related to pH, I'm going to keep it for now. `Mnf Flow` is also correlated to many other variables. It may drop out of a linear model later on.

```{r}
#OLS
data_ctrl <- trainControl(method = 'cv', number = 10)
train1.lm <- train(train_preds2.df, train_resp.df,
                   method = "lm")
summary(train1.lm)
```
Note RMSE = 0.1345.
MAPE: 1.217

```{r}
train1_MAPE <- 100 * (sum(abs(train1.lm$finalModel$residuals) / train_resp.df)) / length(train_resp.df)
```

Before we move on to other models, let's prune variables with p > 0.05.

```{r}
train2.lm <- train(train_preds2.df[,c(3, 8:11, 13:15, 17:21, 23, 24)], train_resp.df, method = "lm")
summary(train2.lm)
```
RMSE = 0.1345.

MAPE: 1.219
```{r}
train2_MAPE <- 100 * (sum(abs(train2.lm$finalModel$residuals) / train_resp.df)) / length(train_resp.df)
```

## Visualize

```{r}
#Actual vs predicted
plot(x = train_resp.df, y = train2.lm$finalModel$fitted.values)

#Predicted vs residual
plot(x = train2.lm$finalModel$fitted.values, y = train2.lm$finalModel$residuals)
```

## Model: Partial Least Squares

```{r}
train.pls <- train(train_preds.df, train_resp.df,
                    method = "pls",
                    tuneLength = 20,
                    trControl = data_ctrl)
```

```{r}
summary(train.pls)
plot(x = train.pls$results$ncomp, y = train.pls$results$RMSE)
```

```{r}
print("RMSE:")
print(train.pls$results$RMSE[13])
print("R^2:")
print(train.pls$results$Rsquared[13])

#If you check out the R^2, you'll see that it's even lower for this model than it was for your regular linear model.
```

```{r}
train.pls_predicted <- predict(train.pls, train_preds.df)

plot(x = train_resp.df, y = train.pls_predicted)
```






```{r}
#Residuals

plot(x = train.pls_predicted, y = train_resp.df - train.pls_predicted)
```

## Model: Elastic Net

```{r}
enetGrid <- expand.grid(.lambda = c(0, 0.01, 0.1), .fraction = seq(0.05, 1, length = 20))
enetTune <- train(train_preds.df[,-31], train_resp.df,
                  method = "enet",
                  tuneGrid = enetGrid,
                  trControl = data_ctrl)
```

## Visualize

```{r}
ggplot(data = enetTune$results, aes(x = fraction, y = RMSE, color = lambda)) +
  geom_point()
```

Minimum RMSE is 0.1387272.

```{r}
ggplot(data = enetTune$results, aes(x = fraction, y = Rsquared, color = lambda)) +
  geom_point()
```

Max r^2 = 0.365.

```{r}
enet_predicted <- predict(enetTune, train_preds.df[,-31])
plot(x = train_resp.df, y = enet_predicted)
```

```{r}
plot(x = enet_predicted, y = train_resp.df - enet_predicted)
```
################
## Test best model, PLS

```{r}
testset_predicted <- predict(train.pls, test_preds.df)
PLS_test <- data.frame(cbind(test_resp.df, testset_predicted))
PLS_test <- PLS_test %>%
  mutate("diff" = testset_predicted - test_resp.df)

PLS_test <- PLS_test %>%
  mutate("sq_diff" = diff^2)

PLS_test_RMSE <- sqrt(sum(PLS_test$sq_diff) / nrow(PLS_test))

print("RMSE:")
print(PLS_test_RMSE)
```

## Predict

```{r}
final_preds.df <- to_predict.df[,-26]

final_brand_code <- final_preds.df[,1]
final_preds.df <- lapply(final_preds.df[,2:ncol(final_preds.df)], function(x) replace(x, is.na(x), median(x, na.rm = TRUE)))
final_preds.df <- as.data.frame(final_preds.df)
final_preds.df$`Brand.Code` <- final_brand_code

#Impute missing Brand Code as "C"
final_brand.code_na <- final_preds.df$Brand.Code == ""
final_preds.df[final_brand.code_na,32] <- "C"

#Drop low-variance variable
final_preds.df <- final_preds.df[,-12]

#Center / scale / Box-Cox
final_trans <- preProcess(final_preds.df, method = c("center", "scale", "BoxCox"))
final_preds.df <- predict(final_trans, final_preds.df)

final_PH <- predict(train.pls, final_preds.df)
```