---
title: "Predicting Beverage pH"
author: "S. Kigamba, L. Li, P. Maloney, D. Moscoe, and D. Moste"
date: "7/17/2021"
output:
  html_document:
    df_print: paged
---


## Introduction

pH is a key performance indicator for the beverage manufacturing process. Because beverage products must maintain a pH within a critical range, it's important to understand how pH relates to other quantifiable aspects of beverage manufacturing. In this report, we seek a model for predicting beverage pH based on data about the beverage itself, along with its manufacturing and bottling process.  

In this report, we select the optimal model, and summarize the steps in the model building process. Our criterion for a successful model is low mean absolute percent error (MAPE) when the model is run on test data. We also consider whether the model provides insight into the most important variables affecting pH. In the sections below, we describe the data, sketch our modeling process, and detail the optimal model for predicting pH. We also describe other models that performed nearly as well as the optimal model. 

## About the data

The data set contains information on 2,571 samples of 24-ounce bottled beverages. Most samples comprise information on 33 variables, such as density, temperature, and pH. Overall, less than 1% of values are missing from the data set. We found no pattern in the missing data.  

With the exception of `Brand Code`, every variable is quantitative. Some variables, especially `Hyd Pressure1`, exhibit low variance. Other variables are highly correlated, which suggests the data set contains some redundant information. We also notice significant skewness in some of the variables. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(corrplot)
library(knitr)
```


```{r}
# Load the datasets
initial_import.df <- read.csv("https://raw.githubusercontent.com/dmoste/DATA624/main/Project2/StudentDataTOMODEL.csv")
to_predict.df <- read.csv("https://raw.githubusercontent.com/dmoste/DATA624/main/Project2/StudentEvaluationTOPREDICT.csv")
```

A correlation plot shows the pairwise correlations in the data set:

```{r}
corr_matrix <- initial_import.df %>%
  keep(is.numeric) %>%
  drop_na() %>%
  cor(method = "pearson")

corrplot::corrplot(corr_matrix, method = "circle", is.corr = TRUE)
```

The response variable, pH, is roughly normally distributed, with mean 8.55 and standard deviation 0.173.  

```{r}
ggplot(data = initial_import.df, aes(x = PH)) +
  geom_histogram() +
  xlab("pH") +
  ylab("Frequency") +
  ggtitle("Response Variable pH is Roughly Normally Distributed")
```

The explanatory variables exhibit a variety of distributions.

```{r}
initial_import.df %>%
  keep(is.numeric) %>%
  gather() %>%
  ggplot(aes(value)) +
  facet_wrap(~ key, scales = "free") + 
  geom_histogram()
```

## Our modeling process

In this report we explore a range of linear models, tree-based models, and neural networks to identify a procedure that is highly accurate in predicting the pH of a previously unseen beverage. For each model, we take the following steps:  

(1) Impute missing data if necessary;
(2) Transform data to address skewness, outliers, and low-variance variables;
(3) Check that data conform to the assumptions of the model;
(4) Fit a model and use cross-validation or another procedure to optimize parameters;
(5) Examine residuals;
(6) Compute model metrics.


## Summary of models

We built six models in total and used MAPE and RMSE scores to evaluate model performance. The summary table below shows the models and their corresponding MAPE and RMSE scores. We noticed that distance and regression tree models performed the best, followed by linear models. Neural Networks (nonlinear) model had the worst performance overall.


```{r results = 'asis'}

Type <- c("OLS", "PLS", "Elastic Net", "KNN", "Neural Nets", "Random Forest")
Parameters <- c("None", "Components = 13", "Lambda = 0, Fraction = 1", "y", "Hidden Units", "ntrees")
MAPE <- c(1.22, 1.19, 1.24, 0.91, 1.34, 0.93)
RMSE <- c(0.135, 0.134, 0.139, 0.11, 0.14, 0.10)

df <- data.frame(Type, Parameters, MAPE, RMSE)
df_sort <- df[with(df, order(MAPE)), ]

kable(df_sort, caption = "Summary of models")

```

## Optimal model: Random Foerst
Our best two performing models are K-Nearest Neighbors and Random Forests, both with a MAPE of < 1.0. In this report we will focus on the Random Forest model. Random Forest model is an ensemble tree-based learning algorithm that averages the prediction over many individual trees. The algorithm uses bootstrap aggregation, or bagging to reduce over fitting and improve accuracy. 

Random Forest models are easy to interpret, and are flexible with both regression and classification problems. They work well with categorical and continuous variables, and can handle large datasets without the data normalization requirement. 


- Go carefully through each step in "Our modeling process."
- Any guesses about why this was the minimum-MAPE model?
- Can you think of any next steps you might take to improve the model even further?


```{r}
data <- read.csv("https://raw.githubusercontent.com/dmoste/DATA624/main/Project2/StudentDataTOMODEL.csv")

library(plyr)
data[,1] <- mapvalues(data[,1],
                      from = c("A","B","C","D",""),
                      to = c(1,2,3,4,NA))
data[,1] <- as.integer(data[,1])

# Removing the response variable since I don't want to impute or transform these values
drops <- c("PH")
features <- data[,!(names(data) %in% drops)]

na_to_mean <- function(x) replace(x, is.na(x), mean(x, na.rm = TRUE))
features[] <- lapply(features, na_to_mean)

processed <- cbind(data[,26], features)
names(processed)[1] <- ("PH")

# Checking if any of the pH data is missing
summary(processed$PH)

processed <- processed[complete.cases(processed),]
```

# Split Data and Train Model

```{r}
set.seed(12345)
train_ind <- sample(seq_len(nrow(processed)),
                    size = floor(0.75*nrow(processed)))

train <- processed[train_ind,]
test <- processed[-train_ind,]
```

lets run a simple random forest model as a baseline

```{r}
library(ggplot2)
library(tidyverse)
library(caret)
library(randomForest)

rf <- randomForest(PH ~ ., data = train, ntrees = 500)
varImpPlot(rf)
rf
test_rf <- predict(rf, test)

caret_test_rf <- data.frame(cbind(test_rf,test[,1]))
colnames(caret_test_rf) <- c("caret","actual")
caret_test_rf <- caret_test_rf %>%
  mutate(pe = abs(actual - caret)/actual)

MAPE <- (mean(caret_test_rf$pe))*100
MAPE

ggplot(caret_test_rf, aes(x = actual, y = caret)) +
  geom_line() +
  geom_point()

```

```{r}
library(gbm)

boosted <- gbm(PH ~ ., data = train, distribution = "gaussian", n.trees = 500, shrinkage = 0.1)
boosted
test_boosted <- predict(boosted, test)

caret_test_boosted <- data.frame(cbind(test_boosted,test[,1]))
colnames(caret_test_boosted) <- c("caret","actual")
caret_test_boosted <- caret_test_boosted %>%
  mutate(pe = abs(actual - caret)/actual)

MAPE <- (mean(caret_test_boosted$pe))*100
MAPE

ggplot(caret_test_boosted, aes(x = actual, y = caret)) +
  geom_line() +
  geom_point()
```

```{r}

features2 <- data[,!(names(data) %in% drops)]

na_to_med <- function(x) replace(x, is.na(x), median(x, na.rm = TRUE))
features2[] <- lapply(features2, na_to_med)

trans <- preProcess(features2,
                    method = c("BoxCox", "center", "scale"))
transformed_feat <- predict(trans, features2)

processed2 <- cbind(data[,26], transformed_feat)
names(processed2)[1] <- ("PH")


processed2 <- processed2[complete.cases(processed2),]

#split data
set.seed(12345)
train_ind <- sample(seq_len(nrow(processed)),
                    size = floor(0.75*nrow(processed)))

train2 <- processed[train_ind,]
test2 <- processed[-train_ind,]

#model

rf2 <- randomForest(PH ~ ., data = train2, ntrees = 500)
varImpPlot(rf2)
rf2
test_rf2 <- predict(rf2, test2)

caret_test_rf2 <- data.frame(cbind(test_rf2,test2[,1]))
colnames(caret_test_rf2) <- c("caret","actual")
caret_test_rf2 <- caret_test_rf2 %>%
  mutate(pe = abs(actual - caret)/actual)

MAPE2 <- (mean(caret_test_rf2$pe))*100
MAPE2

ggplot(caret_test_rf2, aes(x = actual, y = caret)) +
  geom_line() +
  geom_point()
```

The model with the transformed predictor variables and median imputation produced the same MAPE value as the baseline model. This makes some sense since the random forest algorithm is based on partitioning of the data by certain variable values. 

Predictions

```{r}
data2 <- read.csv("https://raw.githubusercontent.com/dmoste/DATA624/main/Project2/StudentEvaluationTOPREDICT.csv")

data2[,1] <- mapvalues(data2[,1],
                      from = c("A","B","C","D",""),
                      to = c(1,2,3,4,NA))
data2[,1] <- as.integer(data2[,1])

# Removing the response variable since I don't want to impute or transform these values
drops <- c("PH")
features3 <- data2[,!(names(data2) %in% drops)]

na_to_mean <- function(x) replace(x, is.na(x), mean(x, na.rm = TRUE))
features3[] <- lapply(features3, na_to_mean)


preds <- predict(rf, features3)

#preds

```

## Other models
- link to repo 

## Ordinary Least Squares

```{r include = FALSE}
library(tidyverse)
library(caret)
library(corrplot)
library(pls)
set.seed(0707)
```

## Impute missing data

```{r}
#Import

initial_import.df <- read.csv("https://raw.githubusercontent.com/dmoste/DATA624/main/Project2/StudentDataTOMODEL.csv")
to_predict.df <- read.csv("https://raw.githubusercontent.com/dmoste/DATA624/main/Project2/StudentEvaluationTOPREDICT.csv")

#Drop missing PH rows
initial_import.df <- initial_import.df %>%
  filter(!is.na(PH))

#Separate predictors, response
preds.df <- initial_import.df[,-26]
resp.df <- initial_import.df[,26]
```

```{r}
#Impute missing values with medians

brand_code <- preds.df[,1]
preds.df <- lapply(preds.df[,2:ncol(preds.df)], function(x) replace(x, is.na(x), median(x, na.rm = TRUE)))
preds.df <- as.data.frame(preds.df)
preds.df$`Brand.Code` <- brand_code

#Impute missing Brand Code as "C"

brand.code_na <- preds.df$Brand.Code == ""
preds.df[brand.code_na,32] <- "C"
```

## Transform data

```{r}
#Drop low-variance variable
preds.df <- preds.df[,-12]

#Center / scale / Box-Cox
trans <- preProcess(preds.df, method = c("center", "scale", "BoxCox"))
preds.df <- predict(trans, preds.df)

#Split into train/test
training_rows <- sample(nrow(preds.df), nrow(preds.df) * 0.80, replace = FALSE)
train_preds.df <- preds.df[training_rows,]
train_resp.df <- resp.df[training_rows]
test_preds.df <- preds.df[-training_rows,]
test_resp.df <- resp.df[-training_rows]
```


## Check model assumptions

One important assumption for ordinary least squares models is that variables are uncorrelated with each other. To check this assumption, we search for highly correlated variables. While removing these variables doesn't guarantee an absence of multicollinearity, it is a useful first step, and can reduce the total number of variables in the model.  

Since this section will be about linear models, let's search for highly correlated variables. While removing these variables doesn't guarantee an absence of multicollinearity, it is a useful first step, and can reduce the total number of variables in the model.

```{r}
corr_matrix <- cbind(train_preds.df, train_resp.df) %>%
  keep(is.numeric) %>%
  cor(method = "pearson")
corrplot::corrplot(corr_matrix, method = "circle", is.corr = TRUE)
```

Groups of variables that are highly correlated:  
`Alch Rel` `Density`, `Balling`, `Carb Rel`, `Balling Lvl`;  
`Air Pressurer` `Carb Rel`, `Balling Lvl`;  
`Bowl Setpoint`, `Filler Level`;  
`Filler Speed`, `MFR`;  
`Hyd Pressure2`, `Hyd Pressure3`;  
`Carb Temp`, `Carb Pressure`.  

To avoid collinearity in a linear model, we eliminate some of the most highly correlated variables:  
Keep `Balling` and drop `Balling Lvl` and Density;  
Keep `Alch Rel` and drop `Carb Rel`;  
Keep `Bowl Setpoint` and drop `Filler Level`;  
Keep `MFR` and drop `Filler Speed`;  
Keep `Hyd Pressure2` and drop `Hyd Pressure3`;  
Keep `Carb Pressure` and drop `Carb Temp`.  

```{r}
train_preds2.df <- train_preds.df %>%
  select(-`Balling.Lvl`,
         -Density,
         -`Carb.Rel`,
         -`Filler.Level`,
         -`Filler.Speed`,
         -`Hyd.Pressure3`,
         -`Carb.Temp`)

corr_matrix <- cbind(train_preds2.df, train_resp.df) %>%
  keep(is.numeric) %>%
  cor(method = "pearson")

corrplot::corrplot(corr_matrix, method = "circle", is.corr = TRUE)

```

There are still some large correlations remaining, for example, between `Balling` and `Alch Rel`. But because I know Balling is a measure of sugar content, and I expect that sugar content is related to pH, I'm going to keep it for now. `Mnf Flow` is also correlated to many other variables. It may drop out of a linear model later on.  

Another assumption for fitting the OLS linear model are that predictors are normally distributed. By applying a Box-Cox transformation to the data in the **Transform Data** section above, we make sure the data roughly conforms to this assumption.  

After fitting a model, we'll check the final assumption-- that residuals have mean zero with approximately constant variance.  

## Fit model

```{r}
#OLS
data_ctrl <- trainControl(method = 'cv', number = 10)
train1.lm <- train(train_preds2.df, train_resp.df,
                   method = "lm")
summary(train1.lm)
```

```{r}
train1_MAPE <- 100 * (sum(abs(train1.lm$finalModel$residuals) / train_resp.df)) / length(train_resp.df)
```

This initial model has MAPE = 1.217 and RMSE 0.1345. We refine the model by removing variables that have low explanatory power (p > 0.05).

```{r}
train2.lm <- train(train_preds2.df[,c(3, 8:11, 13:15, 17:21, 23, 24)], train_resp.df, method = "lm")
summary(train2.lm)
```

```{r}
train2_MAPE <- 100 * (sum(abs(train2.lm$finalModel$residuals) / train_resp.df)) / length(train_resp.df)
```

The refined model has MAPE = 1.219 and RMSE = 0.1345.

After dropping variables with high p-values, the simpler model retains almost all its explanatory power.

```{r}
#Actual vs predicted
plot(x = train_resp.df, y = train2.lm$finalModel$fitted.values,
     xlab = "Actual pH values for training set",
     ylab = "Fitted pH values for linear model",
     main = "Fitted vs Actual pH values for training set")
```

The plot of fitted vs actual values for the training data shows a clear positive linear relationship, although variability is large. The large variability corresponds to the relatively low value of $R^2 = 0.40$.

## Examine residuals

```{r}
#Predicted vs residual
plot(x = train2.lm$finalModel$fitted.values, y = train2.lm$finalModel$residuals,
     xlab = "Fitted pH values for linear model",
     ylab = "Residuals from linear model",
     main = "Residuals vs Fitted pH values for training set")
```

The residuals appear to be randomly distributed with roughly constant variability about a mean of zero.

## Compute model metrics

For the refined model, MAPE = 1.217, and RMSE = 0.1345.

# Partial Least Squares

## Check model assumptions

Because many of the variables in this data set exhibit high correlation, using this data with linear models risks violating the assumption of no multicollinearity. One way to deal with the risk of multicollinearity is to employ a model that performs feature selection, such as partial least squares. Partial least squares performs a kind of feature selection, because it generates new uncorrelated predictors based on "underlying... relationships among the predictors which are highly correlated with the response (Kuhn and Johnson 114). The other modeling assumptions are addressed by transforming the data as described in the corresponding section for OLS. 

## Fit model

```{r}
train.pls <- train(train_preds.df, train_resp.df,
                    method = "pls",
                    tuneLength = 20,
                    trControl = data_ctrl)
```

```{r}
summary(train.pls)
```

Examining the actual and predicted values of the response variable:

```{r}
train.pls_predicted <- predict(train.pls, train_preds.df)

plot(x = train_resp.df, y = train.pls_predicted,
     xlab = "Actual pH values for training set",
     ylab = "PLS Fitted pH values for training set",
     main = "Fitted vs actual pH values for training set")
```

There is a positive linear relationship among the actual and fitted values for the PLS model. This suggests a linear model like PLS is appropriate for this data set.

## Examine residuals

```{r}
#Residuals

plot(x = train.pls_predicted, y = train_resp.df - train.pls_predicted,
     xlab = "Fitted pH values for PLS model",
     ylab = "Residuals from PLS model",
     main = "PLS Residuals vs Fitted pH values for training set")
```

The residuals appears to be randomly distributed with roughly constant variability about a mean of zero.

## Compute model metrics

What is the optimal number of components for the PLS model?

```{r}
summary(train.pls)
plot(x = train.pls$results$ncomp, y = train.pls$results$RMSE,
     xlab = "Number of components in PLS model",
     ylab = "RMSE",
     main = "RMSE declines then stabilizes with increase in PLS components")
```

The optimal number of components for the PLS model is 13. 

```{r}
print("RMSE:")
print(train.pls$results$RMSE[13])
print("R^2:")
print(train.pls$results$Rsquared[13])

PLS_resid <- train.pls$finalModel$residuals[, 1, 13]
train_PLS_MAPE <- (100 / length(train_resp.df)) * sum(abs(PLS_resid /train_resp.df))

print("MAPE:")
print(train_PLS_MAPE)
```

MAPE for the PLS model is 1.19, and RMSE = 0.134.  

# Elastic Net

An elastic net model is another linear modeling method that performs feature selection and is robust to multicollinearity. The elastic net model combines a ridge penalty with a lasso penalty on model coefficients to improve the overall stability of the model. Elastic net models tolerate some increase in coefficient bias in order to reduce variance. Here, we search a range of lasso and ridge parameters to determine an optimal model. 

## Fit model

```{r}
enetGrid <- expand.grid(.lambda = c(0, 0.01, 0.1), .fraction = seq(0.05, 1, length = 20))
enetTune <- train(train_preds.df[,-31], train_resp.df,
                  method = "enet",
                  tuneGrid = enetGrid,
                  trControl = data_ctrl)
```

The optimal model occurs with $\lambda = 0$ and fraction = 1. This is equivalent to a pure lasso model. Including a ridge penalty did not improve model performance.

## Visualize

```{r}
enet_predicted <- predict(enetTune, train_preds.df[,-31])
plot(x = train_resp.df, y = enet_predicted,
     xlab = "Actual pH values",
     ylab = "Fitted pH values for Elastic Net model",
     main = "Actual vs Fitted pH values for Elastic Net model")
```

The relationship between actual and fitted values is linear with positive slope. It demonstrates constant but large variability, consistent with a relatively low value of $R^2 = 0.365$.

## Examine residuals

```{r}
plot(x = enet_predicted, y = train_resp.df - enet_predicted,
     xlab = "Fitted pH Values for Elastic Net",
     ylab = "Residuals from Elastic Net Model",
     main = "Residuals vs Fitted pH Values for Elastic Net")
```

The residuals appear to be randomly distributed about a mean of zero. Variability appears to be largest near fitted values around 8.6.

## Compute model metrics

```{r}
ggplot(data = enetTune$results, aes(x = fraction, y = RMSE, color = lambda)) +
  geom_point() +
  xlab("Fraction of full solution") +
  ylab("RMSE") +
  ggtitle("Optimal RMSE = 0.139 for pure lasso model with all predictors")
```

Minimum RMSE is 0.1387272.  

```{r}
ggplot(data = enetTune$results, aes(x = fraction, y = Rsquared, color = lambda)) +
  geom_point() +
  xlab("Fraction of full solution") +
  ylab("R-Squared") +
  ggtitle("R-squared is maximized for lam = 0, frac = 1")
```

For the optimal elastic net model, $R^2 = 0.365$.

```{r}
#MAPE for Elastic Net:

enet_MAPE <- (100 / length(train_resp.df)) * sum(abs((train_resp.df - enet_predicted) / train_resp.df))

print("MAPE for elastic net:")
print(enet_MAPE)
```

For the optimal elastic net model, MAPE = 1.236.

## Test best linear model, PLS

The model with the best performance among those examined here is PLS. Here we test the performance of the PLS model on the holdout data.

```{r}
testset_predicted <- predict(train.pls, test_preds.df)
PLS_test <- data.frame(cbind(test_resp.df, testset_predicted))
PLS_test <- PLS_test %>%
  mutate("diff" = testset_predicted - test_resp.df)

PLS_test <- PLS_test %>%
  mutate("sq_diff" = diff^2)

PLS_test_RMSE <- sqrt(sum(PLS_test$sq_diff) / nrow(PLS_test))

print("RMSE:")
print(PLS_test_RMSE)
```

The PLS model performs similarly on the holdout data as it did on the test set. This provides evidence that the model has not been over-fitted to the training data.

## Conclusion

- Was there a clear winner, or did several models perform roughly equally?
- Any data we wish we had, but don't?
{"mode":"full","isActive":false}