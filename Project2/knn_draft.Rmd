---
title: "KNN"
author: "David Moste"
date: "7/7/2021"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Pre-Process

Let's start by bringing in the data and getting a sense of what's there.

```{r}
data <- read.csv("https://raw.githubusercontent.com/dmoste/DATA624/main/Project2/StudentDataTOMODEL.csv")

summary(data)

cc <- complete.cases(data)
length(cc[cc == FALSE])/length(cc)

missing_data <- sapply(data, function(x) sum(is.na(x)))
missing_data <- data.frame(missing_data)
(sum(missing_data)/(2571*33))*100

unique(data[,1])
length(data[data[,1] == "",])
```

Clearly there is some missing data, so that needs to be dealt with first. Since only 0.8% of the data is missing, I think there is enough full data that I can just fill in missing values with the mean for each predictor. To do this, I first have to map the Brand.Code predictor into numerical values.

```{r}
library(plyr)
data[,1] <- mapvalues(data[,1],
                      from = c("A","B","C","D",""),
                      to = c(1,2,3,4,NA))
data[,1] <- as.integer(data[,1])

# Removing the response variable since I don't want to impute these values
drops <- c("PH")
features <- data[ ,!(names(data) %in% drops)]

na_to_mean <- function(x) replace(x, is.na(x), mean(x, na.rm = TRUE))
features[] <- lapply(features, na_to_mean)
```

Check to ensure that the data has been completed.

```{r}
cc <- complete.cases(features)
length(cc[cc == FALSE])/length(cc)

missing_data <- sapply(features, function(x) sum(is.na(x)))
missing_data <- data.frame(missing_data)
```

Now I'm going to work on getting some sort of visualization of the data.

```{r}
library(ggplot2)
library(tidyverse)

vis_features <- features %>%
  gather(key = "variable", value = "value")

ggplot(data = vis_features, aes(x = value)) +
  geom_bar() +
  facet_wrap(variable ~ ., scales = "free")
```

KNN is HIGHLY sensitive to the scale of predictors, so I'm going to go ahead and center and scale the predictors before building a model.

```{r}
library(caret)

trans <- preProcess(features,
                    method = c("BoxCox", "center", "scale"))
transformed_feat <- predict(trans, features)
```

Next I'm going to check the variance of each predictor variable and remove anything with near zero variance.

```{r}
library(caret)

nzv <- nearZeroVar(transformed_feat, saveMetrics = TRUE)
nzv[nzv[,"nzv"] == TRUE,]

drops <- c("Hyd.Pressure1")
transformed_feat <- transformed_feat[,!(names(transformed_feat) %in% drops)]
```

Let's add the pH data back into these features and remove any rows that contain NAs (this data isn't usable since we have no idea what the response variable is).

```{r}
processed <- cbind(data[,26], transformed_feat)
names(processed)[1] <- ("PH")

missing_data <- sapply(processed, function(x) sum(is.na(x)))
missing_data <- data.frame(missing_data)

processed <- processed[complete.cases(processed),]
```

# Split Data and Train Model

At this point, I have my features worked the way I want (I think), so I'm going to split the data into a train and test set.

```{r}
set.seed(12345)
train_ind <- sample(seq_len(nrow(processed)),
                    size = floor(0.75*nrow(processed)))

train <- processed[train_ind,]
test <- processed[-train_ind,]
```

Let's go ahead and build a model with the training data!

```{r}
#### train from caret ####
knnModel <- train(train[,-1],
                 train[,1],
                 method = "knn",
                 tuneGrid = data.frame(.k = 1:20),
                 trControl = trainControl(method = "cv"))

knnModel

ggplot(data = knnModel$results, aes(x = k, y = RMSE)) +
  geom_line() +
  geom_point()

# Check best model
knnPred <- predict(knnModel, newdata = test[,-1])

caret_test <- data.frame(cbind(knnPred,test[,1]))
colnames(caret_test) <- c("caret","actual")
caret_test <- caret_test %>%
  mutate(pe = abs(actual - caret)/actual)

MAPE <- (mean(caret_test$pe))*100
MAPE

ggplot(caret_test, aes(x = actual, y = caret)) +
  geom_line() +
  geom_point()
```

The caret package knn regression function found a minimum MAPE of 1.18% with a k value of 6.

Let's see if I can do better with a different package and more control. Trying to model again with the knn.reg function from the FNN package. This package seems interesting because it allows you to change the algorithm, which is a good check on the data size.

```{r}
#### knn.reg from FNN ####
library(FNN)

fnn_func <- function(train_x, train_y, test_x, test_y){
  mape_df <- data.frame(matrix(nrow = 0, ncol = 3))
  
  for(x in c("kd_tree","cover_tree","brute")){
    for(i in 1:20){
      knn_fnn <- knn.reg(train = train_x,
                         test = test_x,
                         y = train_y,
                         k = i,
                         algorithm = x)
      
      mape <- mean(abs(test_y - knn_fnn$pred)/test_y)*100
      mape_df <- rbind(mape_df,c(i,mape,x))
    }
  }
  colnames(mape_df) <- c("k", "MAPE","Type")
  mape_df[,1] <- as.integer(mape_df[,1])
  mape_df[,2] <- as.numeric(mape_df[,2])
  return(mape_df)
}

fnn_mape <- fnn_func(train[,-1], train[,1], test[,-1], test[,1])

ggplot(data = fnn_mape, aes(x = k, y = MAPE, color = Type)) +
  geom_line() +
  geom_point()

# Check best model
fnn_fit <- knn.reg(train = train[,-1],
                   test = test[,-1],
                   y = train[,1],
                   k = 6,
                   algorithm = "kd_tree")

fnn_test <- data.frame(cbind(fnn_fit$pred,test[,1]))
colnames(fnn_test) <- c("fnn","actual")

ggplot(fnn_test, aes(x = actual, y = fnn)) +
  geom_line() +
  geom_point()
```

The FNN knn regression gave a minimum MAPE of 1.18% with a k value of 6.This is the same as the caret package model. It seems there are no differences in any of the algorithms within this package, which would indicate that the dataset is rather small. Since there is no decrease in performance for either kd_tree or cover_tree compared to brute, the data is small enough to compute with just the brute method.

I figured I'd try one more package. This time I'm using the kknn package which allows for weighting the nearest neighbors.

```{r}
#### kknn from kknn ####

library(kknn)

kknn_func <- function(train_x, train_y, test_x, test_y){
  mape_df <- data.frame(matrix(nrow = 0, ncol = 3))
  
  types <- c("rectangular","triangular",
             "biweight","triweight",
             "epanechnikov","optimal")
  
  for(x in types){
    for(i in 2:30){
      ph_kknn <- kknn(train_y ~ .,
                      train_x,
                      test_x,
                      k = i,
                      distance = 1,
                      kernel = x)
      
      mape <- mean(abs(test_y - ph_kknn$fitted.values)/test_y)*100
      mape_df <- rbind(mape_df,c(i,mape,x))
    }
  }
  colnames(mape_df) <- c("k", "MAPE","Type")
  mape_df[,1] <- as.integer(mape_df[,1])
  mape_df[,2] <- as.numeric(mape_df[,2])
  return(mape_df)
}

kknn_mape <- kknn_func(train[,-1], train[,1], test[,-1], test[,1])

ggplot(data = kknn_mape, aes(x = k, y = MAPE, color = Type)) +
  geom_line() +
  geom_point()

# Check best model
kknn_fit <- kknn(train[,1] ~ .,
                 train[,-1],
                 test[,-1],
                 k = 18,
                 distance = 1,
                 kernel = "triweight")

kknn_test <- data.frame(cbind(kknn_fit$fitted.values,test[,1]))
colnames(kknn_test) <- c("kknn","actual")

ggplot(kknn_test, aes(x = actual, y = kknn)) +
  geom_line() +
  geom_point()
```

This model found a minimum MAPE of 0.958% for a triweight kernel at a distance of 1 (Manhattan distance) with a k value of 18. This is definitely the best MAPE I've achieved so far, so I think I'm gonna stick with this model.

The question I have is whether to use a triweight model with k = 18 or a biweight model with k = 12, which is only 0.1% worse (according to MAPE scores). The triweight model seems more complicated to me, though I'm not sure if it really is.

# Hone the top model

I'm going to resample my data and create a new model using the kknn package (my favorite) and see which model is the best this time. Hopefully, this will provide some insight on what to chosoe for my predictions. I'm going to narrow it down to just biweight and triweight this time.

```{r}
library(kknn)

# Re-sample the data
set.seed(54321)
train_ind2 <- sample(seq_len(nrow(processed)),
                    size = floor(0.75*nrow(processed)))

train2 <- processed[train_ind2,]
test2 <- processed[-train_ind2,]

# Try out the kknn models again with the new train and test sets
kknn_func <- function(train_x, train_y, test_x, test_y){
  mape_df <- data.frame(matrix(nrow = 0, ncol = 3))
  
  types <- c("biweight","triweight")
  
  for(x in types){
    for(i in 2:30){
      ph_kknn <- kknn(train_y ~ .,
                      train_x,
                      test_x,
                      k = i,
                      distance = 1,
                      kernel = x)
      
      mape <- mean(abs(test_y - ph_kknn$fitted.values)/test_y)*100
      mape_df <- rbind(mape_df,c(i,mape,x))
    }
  }
  colnames(mape_df) <- c("k", "MAPE","Type")
  mape_df[,1] <- as.integer(mape_df[,1])
  mape_df[,2] <- as.numeric(mape_df[,2])
  return(mape_df)
}

kknn_mape <- kknn_func(train2[,-1], train2[,1], test2[,-1], test2[,1])

ggplot(data = kknn_mape, aes(x = k, y = MAPE, color = Type)) +
  geom_line() +
  geom_point()
```

This time, triweight was still the winner, but with a k value of 9 (and a MAPE of 0.85%). This has me leaning towards a triweight kernel, but I still need to decide on a k value. Let's try re-sampling many times on just a triweight kernel.

```{r}
library(kknn)

# Changing the kkhn function to accept seed values and only run a triweight model on Manhattan distance
kknn_func <- function(train_x, train_y, test_x, test_y, seed){
  mape_df <- data.frame(matrix(nrow = 0, ncol = 3))
  
  for(i in 2:30){
    ph_kknn <- kknn(train_y ~ .,
                    train_x,
                    test_x,
                    k = i,
                    distance = 1,
                    kernel = "triweight")
    
    mape <- mean(abs(test_y - ph_kknn$fitted.values)/test_y)*100
    mape_df <- rbind(mape_df,c(i,mape,seed))
  }
  
  colnames(mape_df) <- c("k", "MAPE","Seed")
  mape_df[,1] <- as.integer(mape_df[,1])
  mape_df[,2] <- as.numeric(mape_df[,2])
  return(mape_df)
}

# Re-sample the data with 7 different test/train sets
kknn_mape <- data.frame(matrix(nrow = 0, ncol = 3))
seeds <- c(1234567,2345671,3456712,4567123,5671234,6712345,7123456)

for(i in seeds){
  print(i)
  
  set.seed(i)
  train_ind <- sample(seq_len(nrow(processed)),
                      size = floor(0.75*nrow(processed)))
  
  train <- processed[train_ind,]
  test <- processed[-train_ind,]
  
  current_mape <- kknn_func(train[,-1], train[,1], test[,-1], test[,1], i)
  kknn_mape <- rbind(kknn_mape, current_mape)
}

colnames(kknn_mape) <- c("k", "MAPE","Seed")
kknn_mape[,1] <- as.integer(kknn_mape[,1])
kknn_mape[,2] <- as.numeric(kknn_mape[,2])
kknn_mape[,3] <- as.factor(kknn_mape[,3])

ggplot(data = kknn_mape, aes(x = k, y = MAPE, color = Seed)) +
  geom_line() +
  geom_point()

# Check which value of k performs the best on average
mape_mean <- aggregate(kknn_mape[,2], list(kknn_mape$k), mean)
```

It looks like the best values for MAPE are all really close. K values between 17 and 20 all produce a MAPE of approximately 0.905%, with k = 18 being the best value by the slimmest of margins.

# Make Predictions

Finally, lets bring in the data we need to predict and go for it! I'll apply all the same methods to the prediction data and see what happens :)

```{r}
library(plyr)
library(caret)
library(kknn)

# Read in the data
predict_df <- read.csv("https://raw.githubusercontent.com/dmoste/DATA624/main/Project2/StudentEvaluationTOPREDICT.csv")

# Check for missing data
missing_data <- sapply(predict_df, function(x) sum(is.na(x)))
missing_data <- data.frame(missing_data)

# Remove PH and Hyd.Pressure1 from the features data
drops <- c("PH", "Hyd.Pressure1")
predict_features <- predict_df[,!(names(predict_df) %in% drops)]

# Map Brand.Code values to numerical options
predict_features[,1] <- mapvalues(predict_features[,1],
                                  from = c("A","B","C","D",""),
                                  to = c(1,2,3,4,NA))
predict_features[,1] <- as.integer(predict_features[,1])

# Replace missing values with the mean of the predictor
na_to_mean <- function(x) replace(x, is.na(x), mean(x, na.rm = TRUE))
predict_features[] <- lapply(predict_features, na_to_mean)

# Apply BoxCox transformations, center the data, and scale it
trans <- preProcess(predict_features,
                    method = c("BoxCox", "center", "scale"))
transformed_feat <- predict(trans, predict_features)

# Recombine the PH response with the transformed features
predict_df <- cbind(predict_df[,26], transformed_feat)
names(predict_df)[1] <- ("PH")

# Train and predict using the model decided from the modeling data
kknn_fit <- kknn(train[,1] ~ .,
                train[,-1],
                predict_df[,-1],
                k = 18,
                distance = 1,
                kernel = "triweight")

kknn_fit$fitted.values
predictions <- data.frame(kknn_fit$fitted.values)
```





