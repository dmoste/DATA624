---
title: "KNN"
author: "David Moste"
date: "7/7/2021"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Let's start by bringing in the data and getting a sense of what's there.

```{r}
data <- read.csv("https://raw.githubusercontent.com/dmoste/DATA624/main/Project2/StudentDataTOMODEL.csv")

summary(data)

cc <- complete.cases(data)
length(cc[cc == FALSE])/length(cc)

missing_data <- sapply(data, function(x) sum(is.na(x)))
missing_data <- data.frame(missing_data)
(sum(missing_data)/(2571*33))*100

unique(data[,1])
length(data[data[,1] == "",])
```

Clearly there is some missing data, so that needs to be dealt with first. Since only 0.8% of the data is missing, I think there is enough full data that I can just fill in missing values with the mean for each predictor. To do this, I first have to map the Brand.Code predictor into numerical values.

```{r}
library(plyr)
data[,1] <- mapvalues(data[,1],
                      from = c("A","B","C","D",""),
                      to = c(1,2,3,4,NA))
data[,1] <- as.integer(data[,1])

# Removing the response variable since I don't want to impute these values
drops <- c("PH")
features <- data[ ,!(names(data) %in% drops)]

na_to_mean <- function(x) replace(x, is.na(x), mean(x, na.rm = TRUE))
features[] <- lapply(features, na_to_mean)
```

Check to ensure that the data has been completed.

```{r}
cc <- complete.cases(features)
length(cc[cc == FALSE])/length(cc)

missing_data <- sapply(features, function(x) sum(is.na(x)))
missing_data <- data.frame(missing_data)
```

Now I'm going to work on getting some sort of visualization of the data.

```{r}
library(ggplot2)
library(tidyverse)

vis_features <- features %>%
  gather(key = "variable", value = "value")

ggplot(data = vis_features, aes(x = value)) +
  geom_bar() +
  facet_wrap(variable ~ ., scales = "free")
```

KNN is HIGHLY sensitive to the scale of predictors, so I'm going to go ahead and center and scale the predictors before building a model.

```{r}
library(caret)

trans <- preProcess(features,
                    method = c("BoxCox", "center", "scale"))
transformed_feat <- predict(trans, features)
```

Next I'm going to check the variance of each predictor variable and remove anything with near zero variance.

```{r}
library(caret)

nzv <- nearZeroVar(transformed_feat, saveMetrics = TRUE)
nzv[nzv[,"nzv"] == TRUE,]

drops <- c("Hyd.Pressure1")
transformed_feat <- transformed_feat[,!(names(transformed_feat) %in% drops)]
```

Let's add the pH data back into these features and remove any rows that contain NAs (this data isn't usable since we have no idea what the response variable is).

```{r}
processed <- cbind(data[,26], transformed_feat)
names(processed)[1] <- ("PH")

missing_data <- sapply(processed, function(x) sum(is.na(x)))
missing_data <- data.frame(missing_data)

processed <- processed[complete.cases(processed),]
```

At this point, I have my features worked the way I want (I think), so I'm going to split the data into a train and test set.

```{r}
set.seed(12345)
train_ind <- sample(seq_len(nrow(processed)),
                    size = floor(0.75*nrow(processed)))

train <- processed[train_ind,]
test <- processed[-train_ind,]
```

Let's go ahead and build a model with the training data!

```{r}
#### train from caret ####
knnModel <- train(train[,-1],
                 train[,1],
                 method = "knn",
                 tuneGrid = data.frame(.k = 1:20),
                 trControl = trainControl(method = "cv"))

knnModel

ggplot(data = knnModel$results, aes(x = k, y = RMSE)) +
  geom_line() +
  geom_point()

ggplot(data = knnModel$results, aes(k)) + 
  geom_line(aes(y = RMSE, color = "RMSE")) + 
  geom_point(aes(y = RMSE)) +
  geom_line(aes(y = MAE, color = "MAE")) +
  geom_point(aes(y = MAE))

# Check best model
knnPred <- predict(knnModel, newdata = test[,-1])

caret_test <- data.frame(cbind(knnPred,test[,1]))
colnames(caret_test) <- c("caret","actual")
caret_test <- caret_test %>%
  mutate(pe = abs(actual - caret)/actual)

MAPE <- (mean(caret_test$pe))*100
MAPE

ggplot(caret_test, aes(x = actual, y = caret)) +
  geom_line() +
  geom_point()
```

The caret package knn regression function found a minimum MAPE of 1.18% with a k value of 6.

Let's see if I can do better with a different package and more control. Trying to model again with the knn.reg function from the FNN package. This package seems interesting because it allows you to change the algorithm, which is a good check on the data size.

```{r}
#### knn.reg from FNN ####
library(FNN)

fnn_func <- function(train_x, train_y, test_x, test_y){
  mape_df <- data.frame(matrix(nrow = 0, ncol = 3))
  
  for(x in c("kd_tree","cover_tree","brute")){
    for(i in 1:20){
      knn_fnn <- knn.reg(train = train_x,
                         test = test_x,
                         y = train_y,
                         k = i,
                         algorithm = x)
      
      mape <- mean(abs(test_y - knn_fnn$pred)/test_y)*100
      mape_df <- rbind(mape_df,c(i,mape,x))
    }
  }
  colnames(mape_df) <- c("k", "MAPE","Type")
  mape_df[,1] <- as.integer(mape_df[,1])
  mape_df[,2] <- as.numeric(mape_df[,2])
  return(mape_df)
}

fnn_mape <- fnn_func(train[,-1], train[,1], test[,-1], test[,1])

ggplot(data = fnn_mape, aes(x = k, y = MAPE, color = Type)) +
  geom_line() +
  geom_point()

# Check best model
fnn_fit <- knn.reg(train = train[,-1],
                   test = test[,-1],
                   y = train[,1],
                   k = 6,
                   algorithm = "kd_tree")

fnn_test <- data.frame(cbind(fnn_fit$pred,test[,1]))
colnames(fnn_test) <- c("fnn","actual")

ggplot(fnn_test, aes(x = actual, y = fnn)) +
  geom_line() +
  geom_point()
```

The FNN knn regression gave a minimum MAPE of 1.18% with a k value of 6.This is the same as the caret package model. It seems there are no differences in any of the algorithms within this package, which would indicate that the dataset is rather small. Since there is no decrease in performance for either kd_tree or cover_tree compared to brute, the data is small enough to compute with just the brute method.

I figured I'd try one more package. This time I'm using the kknn package which allows for weighting the nearest neighbors.

```{r}
#### kknn from kknn ####

library(kknn)

kknn_func <- function(train_x, train_y, test_x, test_y){
  mape_df <- data.frame(matrix(nrow = 0, ncol = 3))
  
  types <- c("rectangular","triangular",
             "biweight","triweight",
             "epanechnikov","gaussian",
             "cos","inv","rank",
             "optimal")
  
  for(x in types){
    for(i in 1:4){
      ph_kknn <- kknn(train_y ~ .,
                      train_x,
                      test_x,
                      distance = i,
                      kernel = x)
      
      mape <- mean(abs(test_y - ph_kknn$fitted.values)/test_y)*100
      mape_df <- rbind(mape_df,c(i,mape,x))
    }
  }
  colnames(mape_df) <- c("Distance", "MAPE","Type")
  mape_df[,1] <- as.integer(mape_df[,1])
  mape_df[,2] <- as.numeric(mape_df[,2])
  return(mape_df)
}

kknn_mape <- kknn_func(train[,-1], train[,1], test[,-1], test[,1])

ggplot(data = kknn_mape, aes(x = Distance, y = MAPE, color = Type)) +
  geom_line() +
  geom_point()

# Check best model
kknn_fit <- kknn(train[,1] ~ .,
                 train[,-1],
                 test[,-1],
                 distance = 1,
                 kernel = "biweight")

kknn_test <- data.frame(cbind(kknn_fit$fitted.values,test[,1]))
colnames(kknn_test) <- c("kknn","actual")

ggplot(kknn_test, aes(x = actual, y = kknn)) +
  geom_line() +
  geom_point()
```

This model found a minimum MAPE of 0.974% for a biweight kernel at a distance of 1 (Manhattan distance). This is definitely the best MAPE I've achieved so far, so I think I'm gonna stick with this model.

Finally, lets bring in the data we need to predict and go for it! I'll apply all the same methods to the prediction data and see what happens :)

```{r}
library(plyr)
library(caret)
library(kknn)

predict_df <- read.csv("https://raw.githubusercontent.com/dmoste/DATA624/main/Project2/StudentEvaluationTOPREDICT.csv")


missing_data <- sapply(predict_df, function(x) sum(is.na(x)))
missing_data <- data.frame(missing_data)

drops <- c("PH", "Hyd.Pressure1")
predict_features <- predict_df[,!(names(predict_df) %in% drops)]

predict_features[,1] <- mapvalues(predict_features[,1],
                                  from = c("A","B","C","D",""),
                                  to = c(1,2,3,4,NA))
predict_features[,1] <- as.integer(predict_features[,1])

na_to_mean <- function(x) replace(x, is.na(x), mean(x, na.rm = TRUE))
predict_features[] <- lapply(predict_features, na_to_mean)

trans <- preProcess(predict_features,
                    method = c("BoxCox", "center", "scale"))
transformed_feat <- predict(trans, predict_features)

predict_df <- cbind(predict_df[,26], transformed_feat)
names(predict_df)[1] <- ("PH")

kknn_fit <- kknn(train[,1] ~ .,
                train[,-1],
                predict_df[,-1],
                distance = 1,
                kernel = "triangular")

kknn_fit$fitted.values
predictions <- data.frame(kknn_fit$fitted.values)
```





