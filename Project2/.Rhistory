rf
test_rf <- predict(rf, test)
caret_test_rf <- data.frame(cbind(test_rf,test[,1]))
colnames(caret_test_rf) <- c("caret","actual")
caret_test_rf <- caret_test_rf %>%
mutate(pe = abs(actual - caret)/actual)
MAPE <- (mean(caret_test_rf$pe))*100
MAPE
library(gbm)
# model witl gaussian distribution
boosted <- gbm(PH ~ ., data = train, distribution = "gaussian", n.trees = 500, shrinkage = 0.1)
boosted
test_boosted <- predict(boosted, test)
caret_test_boosted <- data.frame(cbind(test_boosted,test[,1]))
colnames(caret_test_boosted) <- c("caret","actual")
caret_test_boosted <- caret_test_boosted %>%
mutate(pe = abs(actual - caret)/actual)
MAPE <- (mean(caret_test_boosted$pe))*100
MAPE
features2 <- data[,!(names(data) %in% drops)]
na_to_med <- function(x) replace(x, is.na(x), median(x, na.rm = TRUE))
features2[] <- lapply(features2, na_to_med)
trans <- preProcess(features2,
method = c("BoxCox", "center", "scale"))
transformed_feat <- predict(trans, features2)
processed2 <- cbind(data[,26], transformed_feat)
names(processed2)[1] <- ("PH")
processed2 <- processed2[complete.cases(processed2),]
#split data
set.seed(12345)
train_ind <- sample(seq_len(nrow(processed)),
size = floor(0.75*nrow(processed)))
train2 <- processed[train_ind,]
test2 <- processed[-train_ind,]
# Random Forest model
rf2 <- randomForest(PH ~ ., data = train2, ntrees = 500)
varImpPlot(rf2)
rf2
test_rf2 <- predict(rf2, test2)
caret_test_rf2 <- data.frame(cbind(test_rf2,test2[,1]))
colnames(caret_test_rf2) <- c("caret","actual")
caret_test_rf2 <- caret_test_rf2 %>%
mutate(pe = abs(actual - caret)/actual)
MAPE2 <- (mean(caret_test_rf2$pe))*100
MAPE2
data2 <- read.csv("https://raw.githubusercontent.com/dmoste/DATA624/main/Project2/StudentEvaluationTOPREDICT.csv")
data2[,1] <- mapvalues(data2[,1],
from = c("A","B","C","D",""),
to = c(1,2,3,4,NA))
data2[,1] <- as.integer(data2[,1])
# Removing the response variable since I don't want to impute or transform these values
drops <- c("PH")
features3 <- data2[,!(names(data2) %in% drops)]
na_to_mean <- function(x) replace(x, is.na(x), mean(x, na.rm = TRUE))
features3[] <- lapply(features3, na_to_mean)
preds <- predict(rf, features3)
library(tidyverse)
library(caret)
library(corrplot)
library(pls)
set.seed(0707)
# Impute missing data
#Import
initial_import.df <- read.csv("https://raw.githubusercontent.com/dmoste/DATA624/main/Project2/StudentDataTOMODEL.csv")
to_predict.df <- read.csv("https://raw.githubusercontent.com/dmoste/DATA624/main/Project2/StudentEvaluationTOPREDICT.csv")
#Drop missing PH rows
initial_import.df <- initial_import.df %>%
filter(!is.na(PH))
#Separate predictors, response
preds.df <- initial_import.df[,-26]
resp.df <- initial_import.df[,26]
#Impute missing values with medians
brand_code <- preds.df[,1]
preds.df <- lapply(preds.df[,2:ncol(preds.df)], function(x) replace(x, is.na(x), median(x, na.rm = TRUE)))
preds.df <- as.data.frame(preds.df)
preds.df$`Brand.Code` <- brand_code
#Impute missing Brand Code as "C"
brand.code_na <- preds.df$Brand.Code == ""
preds.df[brand.code_na,32] <- "C"
#Transform data
#Drop low-variance variable
preds.df <- preds.df[,-12]
#Center / scale / Box-Cox
trans <- preProcess(preds.df, method = c("center", "scale", "BoxCox"))
preds.df <- predict(trans, preds.df)
#Split into train/test
training_rows <- sample(nrow(preds.df), nrow(preds.df) * 0.80, replace = FALSE)
train_preds.df <- preds.df[training_rows,]
train_resp.df <- resp.df[training_rows]
test_preds.df <- preds.df[-training_rows,]
test_resp.df <- resp.df[-training_rows]
corr_matrix <- cbind(train_preds.df, train_resp.df) %>%
keep(is.numeric) %>%
cor(method = "pearson")
corrplot::corrplot(corr_matrix, method = "circle", is.corr = TRUE)
train_preds2.df <- train_preds.df %>%
select(-`Balling.Lvl`,
-Density,
-`Carb.Rel`,
-`Filler.Level`,
-`Filler.Speed`,
-`Hyd.Pressure3`,
-`Carb.Temp`)
corr_matrix <- cbind(train_preds2.df, train_resp.df) %>%
keep(is.numeric) %>%
cor(method = "pearson")
corrplot::corrplot(corr_matrix, method = "circle", is.corr = TRUE)
#OLS
data_ctrl <- trainControl(method = 'cv', number = 10)
train1.lm <- train(train_preds2.df, train_resp.df,
method = "lm")
summary(train1.lm)
train1_MAPE <- 100 * (sum(abs(train1.lm$finalModel$residuals) / train_resp.df)) / length(train_resp.df)
train2.lm <- train(train_preds2.df[,c(3, 8:11, 13:15, 17:21, 23, 24)], train_resp.df, method = "lm")
summary(train2.lm)
train2_MAPE <- 100 * (sum(abs(train2.lm$finalModel$residuals) / train_resp.df)) / length(train_resp.df)
#Actual vs predicted
plot(x = train_resp.df, y = train2.lm$finalModel$fitted.values,
xlab = "Actual pH values for training set",
ylab = "Fitted pH values for linear model",
main = "Fitted vs Actual pH values for training set")
#Predicted vs residual
plot(x = train2.lm$finalModel$fitted.values, y = train2.lm$finalModel$residuals,
xlab = "Fitted pH values for linear model",
ylab = "Residuals from linear model",
main = "Residuals vs Fitted pH values for training set")
train.pls <- train(train_preds.df, train_resp.df,
method = "pls",
tuneLength = 20,
trControl = data_ctrl)
summary(train.pls)
plot(x = train.pls$results$ncomp, y = train.pls$results$RMSE,
xlab = "Number of components in PLS model",
ylab = "RMSE",
main = "RMSE declines then stabilizes with increase in PLS components")
print("RMSE:")
print(train.pls$results$RMSE[13])
print("R^2:")
print(train.pls$results$Rsquared[13])
PLS_resid <- train.pls$finalModel$residuals[, 1, 13]
train_PLS_MAPE <- (100 / length(train_resp.df)) * sum(abs(PLS_resid /train_resp.df))
print("MAPE:")
print(train_PLS_MAPE)
train.pls_predicted <- predict(train.pls, train_preds.df)
plot(x = train_resp.df, y = train.pls_predicted,
xlab = "Actual pH values for training set",
ylab = "PLS Fitted pH values for training set",
main = "Fitted vs actual pH values for training set")
#Residuals
plot(x = train.pls_predicted, y = train_resp.df - train.pls_predicted,
xlab = "Fitted pH values for PLS model",
ylab = "Residuals from PLS model",
main = "PLS Residuals vs Fitted pH values for training set")
library(fractional)
library(elasticnet)
enetGrid <- expand.grid(.lambda = c(0, 0.01, 0.1), .fraction = seq(0.05, 1, length = 20))
enetTune <- train(train_preds.df[,-31], train_resp.df,
method = "enet",
tuneGrid = enetGrid,
trControl = data_ctrl)
ggplot(data = enetTune$results, aes(x = fraction, y = RMSE, color = lambda)) +
geom_point() +
xlab("Fraction of full solution") +
ylab("RMSE") +
ggtitle("Optimal RMSE = 0.139 for pure lasso model with all predictors")
ggplot(data = enetTune$results, aes(x = fraction, y = Rsquared, color = lambda)) +
geom_point() +
xlab("Fraction of full solution") +
ylab("R-Squared") +
ggtitle("R-squared is maximized for lam = 0, frac = 1")
enet_predicted <- predict(enetTune, train_preds.df[,-31])
plot(x = train_resp.df, y = enet_predicted,
xlab = "Actual pH values",
ylab = "Fitted pH values for Elastic Net model",
main = "Actual vs Fitted pH values for Elastic Net model")
#MAPE for Elastic Net:
enet_MAPE <- (100 / length(train_resp.df)) * sum(abs((train_resp.df - enet_predicted) / train_resp.df))
print("MAPE for elastic net:")
print(enet_MAPE)
plot(x = enet_predicted, y = train_resp.df - enet_predicted,
xlab = "Fitted pH Values for Elastic Net",
ylab = "Residuals from Elastic Net Model",
main = "Residuals vs Fitted pH Values for Elastic Net")
testset_predicted <- predict(train.pls, test_preds.df)
PLS_test <- data.frame(cbind(test_resp.df, testset_predicted))
PLS_test <- PLS_test %>%
mutate("diff" = testset_predicted - test_resp.df)
PLS_test <- PLS_test %>%
mutate("sq_diff" = diff^2)
PLS_test_RMSE <- sqrt(sum(PLS_test$sq_diff) / nrow(PLS_test))
print("RMSE:")
print(PLS_test_RMSE)
final_preds.df <- to_predict.df[,-26]
final_brand_code <- final_preds.df[,1]
final_preds.df <- lapply(final_preds.df[,2:ncol(final_preds.df)], function(x) replace(x, is.na(x), median(x, na.rm = TRUE)))
final_preds.df <- as.data.frame(final_preds.df)
final_preds.df$`Brand.Code` <- final_brand_code
#Impute missing Brand Code as "C"
final_brand.code_na <- final_preds.df$Brand.Code == ""
final_preds.df[final_brand.code_na,32] <- "C"
#Drop low-variance variable
final_preds.df <- final_preds.df[,-12]
#Center / scale / Box-Cox
final_trans <- preProcess(final_preds.df, method = c("center", "scale", "BoxCox"))
final_preds.df <- predict(final_trans, final_preds.df)
final_PH <- predict(train.pls, final_preds.df)
## Check model assumptions
train.pls <- train(train_preds.df, train_resp.df,
method = "pls",
tuneLength = 20,
trControl = data_ctrl)
summary(train.pls)
train.pls_predicted <- predict(train.pls, train_preds.df)
plot(x = train_resp.df, y = train.pls_predicted,
xlab = "Actual pH values for training set",
ylab = "PLS Fitted pH values for training set",
main = "Fitted vs actual pH values for training set")
#Residuals
plot(x = train.pls_predicted, y = train_resp.df - train.pls_predicted,
xlab = "Fitted pH values for PLS model",
ylab = "Residuals from PLS model",
main = "PLS Residuals vs Fitted pH values for training set")
summary(train.pls)
plot(x = train.pls$results$ncomp, y = train.pls$results$RMSE,
xlab = "Number of components in PLS model",
ylab = "RMSE",
main = "RMSE declines then stabilizes with increase in PLS components")
print("RMSE:")
print(train.pls$results$RMSE[13])
print("R^2:")
print(train.pls$results$Rsquared[13])
PLS_resid <- train.pls$finalModel$residuals[, 1, 13]
train_PLS_MAPE <- (100 / length(train_resp.df)) * sum(abs(PLS_resid /train_resp.df))
print("MAPE:")
print(train_PLS_MAPE)
enetGrid <- expand.grid(.lambda = c(0, 0.01, 0.1), .fraction = seq(0.05, 1, length = 20))
enetTune <- train(train_preds.df[,-31], train_resp.df,
method = "enet",
tuneGrid = enetGrid,
trControl = data_ctrl)
enet_predicted <- predict(enetTune, train_preds.df[,-31])
plot(x = train_resp.df, y = enet_predicted,
xlab = "Actual pH values",
ylab = "Fitted pH values for Elastic Net model",
main = "Actual vs Fitted pH values for Elastic Net model")
plot(x = enet_predicted, y = train_resp.df - enet_predicted,
xlab = "Fitted pH Values for Elastic Net",
ylab = "Residuals from Elastic Net Model",
main = "Residuals vs Fitted pH Values for Elastic Net")
ggplot(data = enetTune$results, aes(x = fraction, y = RMSE, color = lambda)) +
geom_point() +
xlab("Fraction of full solution") +
ylab("RMSE") +
ggtitle("Optimal RMSE = 0.139 for pure lasso model with all predictors")
ggplot(data = enetTune$results, aes(x = fraction, y = Rsquared, color = lambda)) +
geom_point() +
xlab("Fraction of full solution") +
ylab("R-Squared") +
ggtitle("R-squared is maximized for lam = 0, frac = 1")
#MAPE for Elastic Net:
enet_MAPE <- (100 / length(train_resp.df)) * sum(abs((train_resp.df - enet_predicted) / train_resp.df))
print("MAPE for elastic net:")
print(enet_MAPE)
testset_predicted <- predict(train.pls, test_preds.df)
PLS_test <- data.frame(cbind(test_resp.df, testset_predicted))
PLS_test <- PLS_test %>%
mutate("diff" = testset_predicted - test_resp.df)
PLS_test <- PLS_test %>%
mutate("sq_diff" = diff^2)
PLS_test_RMSE <- sqrt(sum(PLS_test$sq_diff) / nrow(PLS_test))
print("RMSE:")
print(PLS_test_RMSE)
library(readxl)
library(skimr)
library(naniar)
library(VIM)
library(MASS)
library(forecast)
library(mixtools)
library(caret)
library(parallel)
library(mlbench)
library(rpart.plot)
library(ggplot2)
library(ggfortify)
library(gridExtra)
library(ggcorrplot)
library(corrplot)
library(RColorBrewer)
library(fpp2)
library(fma)
library(kableExtra)
library(e1071)
library(timeDate)
library(tidyverse)
library(dplyr)
library(tidyr)
library(reshape2)
library(tibble)
library(doParallel)
# Load beverages data set into a dataframe
df = read.csv("https://raw.githubusercontent.com/dmoste/DATA624/main/Project2/StudentDataTOMODEL.csv", header = TRUE)
df_eval = read.csv("https://raw.githubusercontent.com/dmoste/DATA624/main/Project2/StudentEvaluationTOPREDICT.csv", header = TRUE)
# Exclude the empty PH column from the evaluation dataset
df_eval <- df_eval %>%
dplyr::select(-PH)
# Check PH skewness
hist(df$PH)
# Identify missing data by Feature and display percent breakout
missing <- colSums(df %>% sapply(is.na))
missing_pct <- round(missing / nrow(df) * 100, 2)
stack(sort(missing_pct, decreasing = TRUE))
# Various NA plots to inspect data
knitr::kable(miss_var_summary(df),
caption = 'Missing Values',
format="html",
table.attr="style='width:50%;'") %>%
kableExtra::kable_styling()
gg_miss_var(df)
gg_miss_upset(df)
# Prepare data for ggplot
gather_df <- df %>%
drop_na() %>%
dplyr::select(-c(PH, `Brand.Code`)) %>%
gather(key = 'variable', value = 'value')
# Histogram plots of each variable
ggplot(gather_df) +
geom_histogram(aes(x=value, y = ..density..), bins=30) +
geom_density(aes(x=value), color='blue') +
facet_wrap(. ~variable, scales='free', ncol=4)
# Prepare data for ggplot
gather_df <- df %>%
dplyr::select(-c(PH, `Brand.Code`)) %>%
tidyr::drop_na() %>%
gather(key = 'variable', value = 'value')
# Boxplots for each variable
gather_df %>% ggplot() +
geom_boxplot(aes(x=variable, y=value)) +
facet_wrap(. ~variable, scales='free', ncol=6)
# Show feature correlations/target by decreasing correlation
df_features <- df %>%
dplyr::select(-c(PH, `Brand.Code`))
df_features$PH <- df$PH
df_features <- df_features %>%
drop_na
feature_count <- ncol(df_features) - 1
stack(sort(cor(df_features[, feature_count + 1], df_features[,1:feature_count])[,],
decreasing=TRUE))
# Calculate and plot the Multicollinearity
df_features <- df %>%
dplyr::select(-c(`Brand.Code`))
correlation = cor(df_features, use = 'pairwise.complete.obs')
#corrplot(correlation, 'ellipse', type = 'lower', order = 'hclust',
#col=brewer.pal(n=8, name="RdYlBu"))
# Near Zero Variance
nzv <- nearZeroVar(df, saveMetrics= TRUE)
nzv[nzv$nzv,][1:5,] %>% drop_na()
# Remove the fields from our training data
df_clean <- df %>%
dplyr::select(-c(MFR, `Hyd.Pressure1`))
# remove the fields from our evaluation data
df_eval_clean <- df_eval %>%
dplyr::select(-c(MFR, `Hyd.Pressure1`))
set.seed(100)
# drop rows with missing PH
df_clean <- df_clean %>%
filter(!is.na(PH))
# Change Brand.Code missing to 'Unknown' in our training dataset
brand_code <- df_clean %>%
dplyr::select(`Brand.Code`) %>%
replace_na(list(`Brand.Code` = 'Unknown'))
df_clean$`Brand.Code` <- brand_code$`Brand.Code`
# Change Brand.Code missing to 'Unknown' in our evaluation dataset
brand_code <- df_eval_clean %>%
dplyr::select(`Brand.Code`) %>%
replace_na(list(`Brand.Code` = 'Unknown'))
df_eval_clean$`Brand.Code` <- df_eval_clean$`Brand.Code`
# There is an edge case where our Eval data might have a `Brand.Code` not seen in our training set
# If so, let's convert them to 'Unknown'
codes <- unique(df_clean$`Brand.Code`)
df_eval_clean <- df_eval_clean %>%
mutate(`Brand.Code`  = if_else(`Brand.Code` %in% codes, `Brand.Code`, 'Unknown'))
# Use the kNN imputing method to impute missing values
df_clean <- df_clean %>%
kNN(k=10) %>%
dplyr::select(colnames(df_clean))
# Use the kNN imputing method from VIM package to impute missing values in our evaluation data
df_eval_clean <- df_eval_clean %>%
kNN(k=10) %>%
dplyr::select(colnames(df_eval_clean))
# Training data - Convert our `Brand.Code` column into a set of dummy variables
df_clean_dummy <- dummyVars(PH ~ `Brand.Code`, data = df_clean)
dummies <- predict(df_clean_dummy, df_clean)
# Get the dummy column names
dummy_cols <- sort(colnames(dummies))
# Make sure the new dummy columns are sorted in alpha order (to make sure our columns will match the eval dataset)
dummies <- as.tibble(dummies) %>%
dplyr::select(dummy_cols)
# remove the original categorical feature
df_clean <- df_clean %>%
dplyr::select(-`Brand.Code`)
# add the new dummy columns to our main training dataframe
df_clean <- cbind(dummies, df_clean)
# Evaluation data - Convert our `Brand.Code` column into a set of dummy variables
df_eval_clean$PH <- 1
eval_dummies <- predict(df_clean_dummy, df_eval_clean)
# If the eval dataset doesn't have a specific `Brand.Code` lets add dummy columns with all 0's.
for (c in dummy_cols) {
if (!(c %in% colnames(eval_dummies))) {
eval_dummies[c] <- 0
}
}
# Now sort the eval_dummy columns so they match the training set dummies
eval_dummy_cols <- sort(colnames(eval_dummies))
eval_dummies <- as.tibble(eval_dummies) %>%
dplyr::select(eval_dummy_cols)
# remove the original categorical feature
df_eval_clean <- df_eval_clean %>%
dplyr::select(-c(`Brand.Code`, PH))
# add the new dummy columns to our main eval dataframe
df_eval_clean <- cbind(eval_dummies, df_eval_clean)
# Drop the target, PH, we don't want to transform our target,
df_features <- df_clean %>%
dplyr::select(-c(PH))
# Our evaluation (hold out data), note it didn't have the PH column
df_eval_features <- df_eval_clean
# Use caret pre-processing to handle scaling, norm'ing and BoxCox transforming our training data.
preProcValues <- preProcess(
df_features,
method = c("center", "scale", "BoxCox"))
df_transformed <- predict(preProcValues, df_features)
df_transformed$PH <- df_clean$PH
# Do the same for the evaluation data set
df_eval_transformed <- predict(preProcValues, df_eval_features)
preProcValues
training_set <- createDataPartition(df_transformed$PH, p=0.8, list=FALSE)
df_transformed1 <- df_transformed %>% dplyr::select (-PH)
X.train <- df_transformed1[training_set, ]
y.train <- df_transformed$PH[training_set]
X.test <- df_transformed1[-training_set, ]
y.test <- df_transformed$PH[-training_set]
cl <- makePSOCKcluster(5)
registerDoParallel(cl)
set.seed(100)
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
svm_Linear <- train(x = X.train, y = y.train, method = "svmLinear",
trControl=trctrl,
preProcess = c("center", "scale"),
tuneLength = 10)
stopCluster(cl)
svm_Linear$results #summary(svm_Linear)
# Applying Model 1 against our Test Data:
svm_pred <- predict(svm_Linear, newdata = X.test)
test <- data.frame(cbind(svm_pred,y.test))
colnames(test) <- c("test","actual")
test <- test %>%
mutate(pe = abs(actual - test)/actual)
MAPE <- (mean(test$pe))*100
MAPE
ggplot(test, aes(x = actual, y = test)) +
geom_line() +
geom_point()
# Bind results to a table to compare performance of our two models
results <- data.frame()
results <- data.frame(t(postResample(pred = svm_pred, obs = y.test))) %>% mutate(Model = "Support Vector Machine (SVM)") %>% rbind(results)
options(max.print = 1e+06)
cl <- makePSOCKcluster(5)
registerDoParallel(cl)
set.seed(100)
mars_grid <- expand.grid(.degree = 1:2, .nprune = 2:15)
mars_model <- train(x = X.train, y = y.train, method = "earth",
tuneGrid = mars_grid,
preProcess = c("center", "scale"),
tuneLength = 10)
stopCluster(cl)
summary(mars_model)
# Applying Model 2 against our Test Data:
mars_pred <- predict(mars_model, newdata = X.test)
test <- data.frame(cbind(mars_pred, y.test))
colnames(test) <- c("test","actual")
test <- test %>%
mutate(pe = abs(actual - test)/actual)
MAPE <- (mean(test$pe))*100
MAPE
ggplot(test, aes(x = actual, y = test)) +
geom_line() +
geom_point()
# Bind results to a table to compare performance of our two models
results <- data.frame(t(postResample(pred = mars_pred, obs = y.test))) %>% mutate(Model = "Multivariate Adaptive Regression Splines (MARS)") %>% rbind(results)
results
results %>% dplyr::select(Model, RMSE, Rsquared, MAE)
varImp(mars_model)
predictions <- predict(mars_model, df_eval_transformed)
df_eval$PH <- round(predictions, 2)
#write.csv(df_eval, 'eval_predicted.csv', row.names=F)
# Import libraries
library(mice)
library(VIM)
library(lattice)
library(ggplot2)
library(plyr)
library(dplyr)
library(tidyverse)
# Load data
to_model <- read.csv("https://raw.githubusercontent.com/lincarrieli/Data624/main/StudentData%20-%20TO%20MODEL.csv")
to_predict <- read.csv("https://raw.githubusercontent.com/lincarrieli/Data624/main/StudentEvaluation-%20TO%20PREDICT.csv")
# Compute missing values in each column
colSums(is.na(to_model))
# Check number of unique values in Brand.Code column
table(to_model$Brand.Code)
# Assign empty Brand Codes as "A"
to_model$Brand.Code[to_model$Brand.Code == ""] <- "A"
