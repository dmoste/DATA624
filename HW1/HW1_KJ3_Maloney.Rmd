---
title: "HW1_KJ3_Maloney"
author: "Patrick Maloney"
date: "6/26/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(AppliedPredictiveModeling)
library(e1071)
library(mlbench)
library(readxl)
library(tidyverse)
library(VIM)
```
### KJ 3.1

#### Question

"The UC Irvine Machine Learning Repository contains a data set related to glass identification. The data consist of 214 glass samples labeled as one of seven class categories. There are nine predictors, including the refractive index and percentages of eight elements: Na, Mg, Al, Si, K, Ca, Ba, and Fe.

(a) Using visualizations, explore the predictor variables to understand their distributions as well as the relationships between predictors."

#### Code

```{r, warning = FALSE, message = FALSE}
data(Glass)
ggpairs(Glass) +
  ggtitle("Glass: Pairwise scatterplots and correlations")

corrplot::corrplot(cor(Glass[1:9]))

ggplot(data = Glass, aes(x = K)) +
  geom_histogram() +
  ggtitle("Glass: Distribution of K") +
  xlab("K content (% by weight)") +
  ylab("Number of samples")

Glass %>%
  filter(K < 2) %>%
  ggplot(aes(x = K)) +
  geom_histogram() +
  ggtitle("Glass: Distribution of K excluding outliers") +
  xlab("K content (% by weight)") +
  ylab("Number of samples")

ggplot(data = Glass, aes(x = Ba)) +
  geom_histogram() +
  ggtitle("Glass: Distribution of Ba") +
  xlab("Ba content (% by weight)") +
  ylab("Number of samples")

```
#### Response

A visual exploration of our data gives us a quick overview of the distributions of each of the nine variables, as well as how they correlate with each other. The pairwise scatterplots and correlations above, Na, Al, and Si appear approximately normally distributed, with minimal to moderate skew. RI, Mg, and Ca demonstrate significant skew, and may require transformations that mitigate skewness. K, Ba, and Fe appear to be comprised of a large number of low- or zero-values, along with a small number of larger or outlying vales. 

There is a clear linear relationship between the amount of Ca in a sample and its Reflective Index, with a correlation coefficient of 0.81. Si and RI have a correlation coefficient of -0.542. Both these linear correlations are clearly visible in the scatterplots for these pairs of variables. In order to avoid collinearity, it may make sense to drop either Ca or RI from the forecast, given the high correlation between the two.

After taking a deeper dive into the variables that don't follow a normal distribution, we see that the variable K appears bimodally distributed around 0 and 0.6, when outliers are excluded. We also see that Ba is a low-variance variable, with only 38 of 214 samples having a non-zero value.


#### Question

"(b) Do there appear to be any outliers in the data? Are any predictors skewed?"

#### Code

```{r, warning = FALSE, message = FALSE}
skewness <- apply(Glass[,1:9], 2, skewness)
skewness

Glass_features <- Glass[,1:9] %>%
  gather(key = 'variable', value = 'value')

ggplot(Glass_features, aes(variable, value)) +
  geom_boxplot() +
  facet_wrap(. ~ variable, scales = 'free') +
  ggtitle("Glass: Boxplots for independent variables") +
  xlab("variable") +
  ylab("% content by weight")
```
#### Response

Yes, the data contains both outliers and skewness. The skewness table shows us that all the variables contain some degree of skewness, but K, Ca, and Ba are significantly skewed (above a 2.0 threshold). These will need to be transformed. Fe comes in below the threshold, but contains many zero values and also would be suitable for transformation.

According to the boxplots, all the variables contain statistical outliers except Mg. K contains the most significant outlier by far, which should definitely be excluded. Ba, Fe, and K are especially noteworthy, because they combine significant skewness with a large number of outlying values.

#### Question

"(c). Are there any relevant transformations of one or more predictors that might improve the classification model?"

#### Response

Centering and scaling would be an appropriate approach given the orders of magnitude between Si and Fe and Ba. Most of the variables would be suitable for a Box-Cox transformation, but especially Ba, Fe, and K. Also, it may be worth considering excluding Ca from the analysis, given its high collinearity with RI, and high level of skewness. 

### KJ 3.2

#### Question

"The soybean data can also be found at the UC Irvine Machine Learning Repository. DAta were collected to predict disease in 683 soybeans. The 35 predictors are mostly categorical and include information on the environmental conditions (e.g., temperature, precipitation) and plant conditions (e.g., left spots, mold growth). The outcome labels consist of 19 distinct classes."

"(a). Investigate the frequency distributions for the categorical predictors. Are any of the distributions degenerate in the ways discussed earlier in this chapter?"

#### Code

```{r, warning = FALSE, message = FALSE}
data(Soybean)
#summary(Soybean)

nzv <- nearZeroVar(Soybean, saveMetrics = TRUE)
nzv[nzv[,"nzv"] == TRUE,]
```
#### Response

Leaf.mild, myselium, and scleorota all have high frequency ratios (more than 20), combined with a low percentage of unique values vs. the overall samples (less than 10%).

#### Question

"(b). Roughly 18% of the data are missing. Are there particular predictors that are more likely to be missing? Is the pattern of missing data related to the classes?"

#### Code

```{r, warning = FALSE, message = FALSE}
soybean_na <- sapply(Soybean, function(x) sum(is.na(x)))
soybean_na <- data.frame(soybean_na)
arrange(soybean_na, -soybean_na)

par(mfrow = c(1,3))
aggr(Soybean, only.miss=TRUE, sortVars=TRUE)
```

#### Response

The predictors most likely to be missing are hail, sever, seed.tmt, and lodging, each with 121 missing values. As we can see from the missing values plot, there seems to be a pattern with several variables containing the same amount of missing values. For example,in the summation table all variables beginning with `leaf.` exhibit the same number of missing values, 84. And a group of variables that seems to relate to plant diseases all share the same number of missing values, 38. These variables are stem.cankers, canker.lesion, ext.decay, mycelium, int.discolor, sclerotia. This suggests that perhaps there was an issue with certain facets of the data not being measured or uploaded at the same time as other features. 

#### Question

"(c). Develop a strategy for handling missing data, either by eliminating predictors or imputation."

#### Response

With 18 percent of the data containing missing values, simply ignoring missing values would result in too much data loss. Also, given the patterns shown in how the data is missing, it makes sense that there could be some meaning in the way the values are missing. Imputation is likely the best way forward, and since the variables are categorical, KNN is the best option. However, it most be noted that some of the variables have large class-imbalance issues, which could affect the imputation accuracy.






